import torch
import os
from datasets import load_dataset, Dataset, IterableDataset
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from transformers import PreTrainedTokenizerFast
from trl.trainer.sft_trainer import SFTTrainer

from typing import cast, Dict, Any, List

# change hf endpoint
# os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

# 1. é…ç½®å‚æ•°
# ------------------------------------------------------------------------------------
CLUSTER_N = 45  # èšç±»æ•°é‡
MODEL_NAME = "qwen1.5_moe_merged_svd_cluster_" + str(CLUSTER_N)
base_model_path = "/root/fsas/zhanghongyu/SMoE/qwen/merged_models" + "/" + MODEL_NAME
dataset_name = "HuggingFaceH4/ultrafeedback_binarized" 
output_dir = "/root/fsas/zhanghongyu/SMoE/qwen/finetuned_models" + "/" + MODEL_NAME
os.makedirs(output_dir, exist_ok=True)

# 2. åŠ è½½å¹¶é¢„å¤„ç†æ•°æ®é›†
# ------------------------------------------------------------------------------------
print("--- æ­£åœ¨åŠ è½½å’Œé¢„å¤„ç†æ•°æ®é›† ---")
dataset = load_dataset(dataset_name, split="train_sft")

assert isinstance(dataset, Dataset)

print(f"--- åŸå§‹æ•°æ®é›†åˆ—å: {dataset.column_names} ---")
print(f"--- åŸå§‹æ•°æ®é›†æ ·æœ¬æ•°: {len(dataset)} ---")

# ğŸ¯ ç”Ÿäº§é…ç½®ï¼šä½¿ç”¨æ›´å¤šä½†åˆç†çš„æ ·æœ¬æ•°é‡
TRAIN_SAMPLES = 10000  # ä½¿ç”¨1ä¸‡ä¸ªé«˜è´¨é‡æ ·æœ¬ï¼Œè¶³å¤Ÿè¿›è¡Œæ¢å¤æ€§å¾®è°ƒ
print(f"--- ğŸš€ ç”Ÿäº§æ¨¡å¼ï¼šä½¿ç”¨ {TRAIN_SAMPLES} ä¸ªæ ·æœ¬è¿›è¡Œè®­ç»ƒ ---")
dataset = dataset.select(range(min(TRAIN_SAMPLES, len(dataset))))
print(f"--- è®­ç»ƒæ•°æ®é›†æ ·æœ¬æ•°: {len(dataset)} ---")

# é¢„å¤„ç†å‡½æ•°ï¼šåªä¿ç•™ messages åˆ—
def preprocess_dataset(example: Dict[str, Any]) -> Dict[str, Any]:
    """
    åªä¿ç•™ messages åˆ—ï¼Œç§»é™¤å…¶ä»–æ‰€æœ‰åˆ—
    """
    return {"messages": example["messages"]}

# åº”ç”¨é¢„å¤„ç†ï¼Œåªä¿ç•™ messages åˆ—
print("--- å¼€å§‹é¢„å¤„ç†æ•°æ®é›†ï¼Œåªä¿ç•™ messages åˆ— ---")
dataset = dataset.map(
    preprocess_dataset, 
    remove_columns=[col for col in dataset.column_names if col != "messages"],
    num_proc=4,  # ğŸ¯ ä½¿ç”¨å¤šè¿›ç¨‹åŠ é€Ÿé¢„å¤„ç†
    desc="é¢„å¤„ç†æ•°æ®é›†"
)

print(f"--- é¢„å¤„ç†åæ•°æ®é›†åˆ—å: {dataset.column_names} ---")

# 3. åŠ è½½æ¨¡å‹å’ŒTokenizer
# ------------------------------------------------------------------------------------
print("--- åŠ è½½ Tokenizer å’Œæ¨¡å‹ ---")
tokenizer: PreTrainedTokenizerFast = AutoTokenizer.from_pretrained(base_model_path, trust_remote_code=True)
assert isinstance(tokenizer.eos_token, str), "eos_token should be a string"

# æ˜ç¡®é…ç½®ç‰¹æ®Štoken
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.pad_token_id = tokenizer.eos_token_id

# æ‰“å°tokenizeré…ç½®ä¿¡æ¯
print("--- Tokenizeré…ç½® ---")
print(f"EOS token: '{tokenizer.eos_token}' (ID: {tokenizer.eos_token_id})")
print(f"PAD token: '{tokenizer.pad_token}' (ID: {tokenizer.pad_token_id})")

# åŠ è½½æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained(
    base_model_path,
    device_map="auto",
    torch_dtype=torch.bfloat16,
    trust_remote_code=True,
    pad_token_id=tokenizer.pad_token_id,  # æ˜ç¡®æŒ‡å®špad_token_id
)

# å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ä»¥èŠ‚çœæ˜¾å­˜
model.gradient_checkpointing_enable()
print("--- æ¨¡å‹åŠ è½½å®Œæˆï¼Œå·²å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ ---")

# 4. æœ€ä½³ç”Ÿäº§é…ç½® (TrainingArguments)
# ------------------------------------------------------------------------------------
training_args = TrainingArguments(
    output_dir=output_dir,
    
    # ğŸ¯ æ‰¹æ¬¡é…ç½®ï¼šå¹³è¡¡æ˜¾å­˜å’Œæ•ˆç‡
    per_device_train_batch_size=1,        # ä¿æŒä¸º1ï¼Œé¿å…OOM
    gradient_accumulation_steps=8,        # æ¢å¤åˆ°8ï¼Œæœ‰æ•ˆbatch_size=8
    
    # ğŸ¯ å­¦ä¹ ç‡é…ç½®ï¼šæ¢å¤æ€§å¾®è°ƒçš„å…³é”®
    learning_rate=2e-6,                   # è¿›ä¸€æ­¥é™ä½å­¦ä¹ ç‡ï¼Œæ›´ä¿å®ˆçš„æ¢å¤è®­ç»ƒ
    weight_decay=0.01,                    # æ·»åŠ æƒé‡è¡°å‡ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
    
    # ğŸ¯ è®­ç»ƒè½®æ•°é…ç½®
    num_train_epochs=1,                   # æ¢å¤æ€§å¾®è°ƒåªéœ€è¦1è½®
    # max_steps=None,                     # ç§»é™¤æ­¥æ•°é™åˆ¶ï¼Œå®Œæ•´è®­ç»ƒ
    
    # ğŸ¯ å­¦ä¹ ç‡è°ƒåº¦
    lr_scheduler_type="cosine",
    warmup_ratio=0.03,                    # å‡å°‘warmupï¼Œå› ä¸ºæ¨¡å‹å·²ç»è®­ç»ƒè¿‡äº†
    
    # ğŸ¯ æ—¥å¿—å’Œä¿å­˜é…ç½®
    logging_steps=25,                     # é€‚ä¸­çš„æ—¥å¿—é¢‘ç‡
    logging_strategy="steps",
    save_strategy="steps",
    save_steps=250,                       # æ¯250æ­¥ä¿å­˜ä¸€æ¬¡
    save_total_limit=3,                   # ä¿ç•™3ä¸ªcheckpoint
    
    # ğŸ¯ ç²¾åº¦å’Œæ€§èƒ½é…ç½®
    fp16=False,
    bf16=True,                           # ä½¿ç”¨bf16ä»¥è·å¾—æ›´å¥½çš„æ•°å€¼ç¨³å®šæ€§
    max_grad_norm=1.0,                   # å¢åŠ æ¢¯åº¦å‰ªåˆ‡é˜ˆå€¼
    
    # ğŸ¯ æ•°æ®åŠ è½½é…ç½®
    dataloader_num_workers=2,            # é€‚åº¦çš„workeræ•°é‡
    dataloader_pin_memory=True,          # åŠ é€Ÿæ•°æ®ä¼ è¾“
    group_by_length=True,                # æŒ‰é•¿åº¦åˆ†ç»„ï¼Œæé«˜æ•ˆç‡
    
    # ğŸ¯ å…¶ä»–é…ç½®
    remove_unused_columns=False,
    report_to="tensorboard",
    run_name=f"qwen_moe_recovery_cluster_{CLUSTER_N}",  # æ¸…æ™°çš„è¿è¡Œåç§°
    
    # ğŸ¯ æ¢å¤è®­ç»ƒç‰¹å®šé…ç½®
    ignore_data_skip=False,              # å…è®¸ä»checkpointæ¢å¤æ—¶è·³è¿‡å·²å¤„ç†æ•°æ®
    seed=42,                            # å›ºå®šéšæœºç§å­ä»¥æé«˜å¤ç°æ€§
)

# é¢„ä¼°è®­ç»ƒæ—¶é—´å’Œèµ„æº
total_steps = len(dataset) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps)
print("--- è®­ç»ƒé…ç½®è¯¦æƒ… ---")
print(f"ğŸ“Š æ ·æœ¬æ•°é‡: {len(dataset):,}")
print(f"ğŸ“Š æ¯è®¾å¤‡æ‰¹æ¬¡å¤§å°: {training_args.per_device_train_batch_size}")
print(f"ğŸ“Š æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {training_args.gradient_accumulation_steps}")
print(f"ğŸ“Š æœ‰æ•ˆæ‰¹æ¬¡å¤§å°: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}")
print(f"ğŸ“Š é¢„ä¼°æ€»æ­¥æ•°: {total_steps:,}")
print(f"ğŸ“Š å­¦ä¹ ç‡: {training_args.learning_rate}")
print(f"ğŸ“Š é¢„ä¼°è®­ç»ƒæ—¶é—´: {total_steps * 30 / 3600:.1f} å°æ—¶ (å‡è®¾30ç§’/æ­¥)")

# 5. åˆå§‹åŒ– SFTTrainer å¹¶å¼€å§‹è®­ç»ƒ
# ------------------------------------------------------------------------------------
print("--- åˆå§‹åŒ– SFTTrainer ---")

trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
    processing_class=tokenizer,
)

print("--- å¼€å§‹ç”Ÿäº§çº§å…¨å‚æ•°å¾®è°ƒ ---")
print(f"ğŸ’¡ Tensorboard å‘½ä»¤: tensorboard --logdir={output_dir} --port=6006 --bind_all")
print(f"ğŸš€ æ­£å¼è®­ç»ƒæ¨¡å¼ï¼šçº¦ {total_steps:,} æ­¥")
print(f"ğŸ’¾ æ¨¡å‹å°†ä¿å­˜åœ¨: {output_dir}")

try:
    # è®­ç»ƒå¼€å§‹
    trainer.train()
    print("--- âœ… å¾®è°ƒå®Œæˆï¼---")
    
except KeyboardInterrupt:
    print("--- â¹ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­ ---")
    print("ğŸ’¡ å¯ä»¥ä½¿ç”¨ trainer.train(resume_from_checkpoint=True) æ¢å¤è®­ç»ƒ")
    
except torch.cuda.OutOfMemoryError as e:
    print("--- âŒ æ˜¾å­˜ä¸è¶³ ---")
    print("ğŸ’¡ å»ºè®®è°ƒæ•´é…ç½®ï¼š")
    print("   - å‡å°‘ gradient_accumulation_steps åˆ° 4")
    print("   - æˆ–å‡å°‘ TRAIN_SAMPLES åˆ° 5000")
    raise
    
except Exception as e:
    print(f"--- âŒ è®­ç»ƒå‡ºé”™: {e} ---")
    raise

# 6. ä¿å­˜æœ€ç»ˆæ¨¡å‹
# ------------------------------------------------------------------------------------
print("--- ä¿å­˜æœ€ç»ˆæ¨¡å‹ ---")
try:
    # ä¿å­˜å®Œæ•´æ¨¡å‹
    final_model_path = os.path.join(output_dir, "final_model")
    trainer.save_model(final_model_path)
    print(f"--- âœ… å®Œæ•´æ¨¡å‹å·²ä¿å­˜è‡³: {final_model_path} ---")
    
    # ä¿å­˜tokenizer
    tokenizer.save_pretrained(final_model_path)
    print(f"--- âœ… Tokenizerå·²ä¿å­˜è‡³: {final_model_path} ---")
    
    # ä¿å­˜è®­ç»ƒå‚æ•°
    # training_args.save_to_json(os.path.join(final_model_path, "training_args.json"))
    print(f"--- âœ… è®­ç»ƒå‚æ•°å·²ä¿å­˜ ---")
    
except Exception as e:
    print(f"--- âŒ ä¿å­˜å‡ºé”™: {e} ---")
    raise

print("--- ğŸ‰ æ¢å¤æ€§å¾®è°ƒå®Œå…¨å®Œæˆï¼---")
print("ğŸ” æ¨¡å‹è´¨é‡æ£€æŸ¥å»ºè®®ï¼š")
print("1. ä½¿ç”¨æµ‹è¯•æ•°æ®é›†è¯„ä¼°æ¨¡å‹æ€§èƒ½")
print("2. è¿›è¡Œäººå·¥å¯¹è¯æµ‹è¯•")
print("3. å¯¹æ¯”å¾®è°ƒå‰åçš„è¾“å‡ºè´¨é‡")
print("4. æ£€æŸ¥æ˜¯å¦å‡ºç°ç¾éš¾æ€§é—å¿˜")

print(f"ğŸ“ æœ€ç»ˆæ¨¡å‹ä½ç½®: {final_model_path}")
print(f"ğŸ“Š è®­ç»ƒæ—¥å¿—: {output_dir}/runs/")