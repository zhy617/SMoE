import torch
import os
from datasets import load_dataset, Dataset, IterableDataset
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig, Qwen2Tokenizer
from transformers import PreTrainedTokenizerFast
from trl.trainer.sft_trainer import SFTTrainer

from typing import cast, Dict, Any, List

# change hf endpoint
# os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

# 1. é…ç½®å‚æ•°
# ------------------------------------------------------------------------------------
# !!! é‡è¦: å°†æ­¤è·¯å¾„æ›¿æ¢ä¸ºæ‚¨å‹ç¼©åã€åŒ…å«è‡ªå®šä¹‰ä»£ç çš„æ¨¡å‹æ–‡ä»¶å¤¹è·¯å¾„
CLUSTER_N = 45  # èšç±»æ•°é‡
MODEL_NAME = "qwen1.5_moe_merged_svd_cluster_" + str(CLUSTER_N)
base_model_path = "/root/fsas/zhanghongyu/SMoE/qwen/merged_models" + "/" + MODEL_NAME
# ä½¿ç”¨ä¸€ä¸ªå…¬å¼€çš„é«˜è´¨é‡æ•°æ®é›†
dataset_name = "HuggingFaceH4/ultrafeedback_binarized" 
# å¾®è°ƒåæ–°æ¨¡å‹çš„ä¿å­˜è·¯å¾„
output_dir = "/root/fsas/zhanghongyu/SMoE/qwen/finetuned_models" + "/" + MODEL_NAME
os.makedirs(output_dir, exist_ok=True)

# 2. åŠ è½½å¹¶é¢„å¤„ç†æ•°æ®é›†
# ------------------------------------------------------------------------------------
# UltraFeedback æ•°æ®é›†éœ€è¦ç‰¹æ®Šå¤„ç†
print("--- æ­£åœ¨åŠ è½½å’Œé¢„å¤„ç†æ•°æ®é›† ---")
dataset = load_dataset(dataset_name, split="train_sft")

assert isinstance(dataset, Dataset)

# æ˜¾ç¤ºåŸå§‹æ•°æ®é›†ä¿¡æ¯
print(f"--- åŸå§‹æ•°æ®é›†åˆ—å: {dataset.column_names} ---")
print(f"--- åŸå§‹æ•°æ®é›†æ ·æœ¬æ•°: {len(dataset)} ---")

# ğŸ¯ é™åˆ¶æ•°æ®é›†å¤§å°è¿›è¡Œæµ‹è¯•
TEST_SAMPLES = 100  # å…ˆç”¨100ä¸ªæ ·æœ¬æµ‹è¯•
print(f"--- ğŸ§ª æµ‹è¯•æ¨¡å¼ï¼šé™åˆ¶æ•°æ®é›†ä¸º {TEST_SAMPLES} ä¸ªæ ·æœ¬ ---")
dataset = dataset.select(range(min(TEST_SAMPLES, len(dataset))))
print(f"--- æµ‹è¯•æ•°æ®é›†æ ·æœ¬æ•°: {len(dataset)} ---")

# é¢„å¤„ç†å‡½æ•°ï¼šåªä¿ç•™ messages åˆ—
def preprocess_dataset(example: Dict[str, Any]) -> Dict[str, Any]:
    """
    åªä¿ç•™ messages åˆ—ï¼Œç§»é™¤å…¶ä»–æ‰€æœ‰åˆ—
    """
    return {"messages": example["messages"]}

# åº”ç”¨é¢„å¤„ç†ï¼Œåªä¿ç•™ messages åˆ—
print("--- å¼€å§‹é¢„å¤„ç†æ•°æ®é›†ï¼Œåªä¿ç•™ messages åˆ— ---")
dataset = dataset.map(
    preprocess_dataset, 
    remove_columns=[col for col in dataset.column_names if col != "messages"]
)

print(f"--- é¢„å¤„ç†åæ•°æ®é›†åˆ—å: {dataset.column_names} ---")

# æ‰“å°ä¸€ä¸ªæ ·æœ¬æŸ¥çœ‹æ ¼å¼
print("--- æ ·æœ¬ç¤ºä¾‹ ---")
sample = dataset[0]
print(f"Messages ç±»å‹: {type(sample['messages'])}")
print(f"Messages å†…å®¹: {sample['messages'][:2]}...")  # åªæ˜¾ç¤ºå‰2æ¡æ¶ˆæ¯

# 3. åŠ è½½æ¨¡å‹å’ŒTokenizer
# ------------------------------------------------------------------------------------
# Qwençš„Tokenizeréœ€è¦è®¾ç½®pad_token
tokenizer: PreTrainedTokenizerFast = AutoTokenizer.from_pretrained(base_model_path, trust_remote_code=True)
assert isinstance(tokenizer.eos_token, str), "eos_token should be a string"
tokenizer.pad_token = tokenizer.eos_token

# åŠ è½½æ¨¡å‹ï¼Œå› ä¸ºæ˜¯æ¢å¤æ€§å¾®è°ƒï¼Œæˆ‘ä»¬ä¸éœ€è¦é‡åŒ–ï¼Œç›´æ¥ç”¨bfloat16åŠ è½½
# trust_remote_code=True æ˜¯å¿…é¡»çš„ï¼Œå› ä¸ºå®ƒä¼šåŠ è½½æ‚¨æ–‡ä»¶å¤¹ä¸­çš„è‡ªå®šä¹‰æ¨¡å‹ä»£ç 
model = AutoModelForCausalLM.from_pretrained(
    base_model_path,
    device_map="auto",
    torch_dtype=torch.bfloat16,
    trust_remote_code=True, 
)
# å¯¹äºå…¨å‚æ•°å¾®è°ƒï¼Œå»ºè®®å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ä»¥èŠ‚çœæ˜¾å­˜
model.gradient_checkpointing_enable()

# 4. é…ç½®è®­ç»ƒå‚æ•° (TrainingArguments) - é’ˆå¯¹æµ‹è¯•ä¼˜åŒ–
# ------------------------------------------------------------------------------------
training_args = TrainingArguments(
    output_dir=output_dir,
    per_device_train_batch_size=1,        # ä¿æŒæœ€å°batch size
    gradient_accumulation_steps=2,        # ğŸ¯ å‡å°‘ç´¯ç§¯æ­¥æ•°ï¼š2è€Œä¸æ˜¯8
    learning_rate=5e-6,                   # å…¨å‚æ•°å¾®è°ƒçš„ä½å­¦ä¹ ç‡
    num_train_epochs=1,                   # åªè®­ç»ƒ1è½®
    max_steps=10,                         # ğŸ¯ é™åˆ¶æœ€å¤§æ­¥æ•°ï¼Œç”¨äºå¿«é€Ÿæµ‹è¯•
    lr_scheduler_type="cosine",
    logging_steps=2,                      # ğŸ¯ æ›´é¢‘ç¹çš„æ—¥å¿—è®°å½•
    save_strategy="steps",               
    save_steps=5,                         # ğŸ¯ æ›´é¢‘ç¹çš„ä¿å­˜ï¼Œæµ‹è¯•ä¿å­˜åŠŸèƒ½
    save_total_limit=2,                 
    fp16=False, 
    bf16=True,                          
    max_grad_norm=0.3,
    warmup_ratio=0.1,                     # ğŸ¯ å¢åŠ warmupæ¯”ä¾‹
    group_by_length=False,                # ğŸ¯ å…³é—­åˆ†ç»„ä»¥ç®€åŒ–å¤„ç†
    report_to="tensorboard",
    dataloader_num_workers=0,             # ğŸ¯ å‡å°‘workeræ•°é‡
    remove_unused_columns=False,          # ğŸ¯ ä¿ç•™æ‰€æœ‰åˆ—ä»¥é¿å…æ½œåœ¨é—®é¢˜
)

print("--- è®­ç»ƒé…ç½® ---")
print(f"Batch size: {training_args.per_device_train_batch_size}")
print(f"Gradient accumulation: {training_args.gradient_accumulation_steps}")
print(f"Max steps: {training_args.max_steps}")
print(f"æœ‰æ•ˆ batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}")

# 5. åˆå§‹åŒ– SFTTrainer å¹¶å¼€å§‹è®­ç»ƒ
# ------------------------------------------------------------------------------------
print("--- åˆå§‹åŒ– SFTTrainer ---")

trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
    processing_class=tokenizer,
)

print("--- å¼€å§‹å…¨å‚æ•°å¾®è°ƒ ---")
print(f"ğŸ’¡ Tensorboard æ—¥å¿—ç›®å½•: {output_dir}")
print(f"ğŸ’¡ å¯åŠ¨å‘½ä»¤: tensorboard --logdir={output_dir} --port=6006 --bind_all")
print(f"ğŸ¯ æµ‹è¯•æ¨¡å¼ï¼šåªè®­ç»ƒ {training_args.max_steps} æ­¥")

try:
    trainer.train()
    print("--- âœ… å¾®è°ƒå®Œæˆ ---")
except Exception as e:
    print(f"--- âŒ è®­ç»ƒå‡ºé”™: {e} ---")
    raise

# 6. ä¿å­˜æœ€ç»ˆæ¨¡å‹
# ------------------------------------------------------------------------------------
print("--- å¼€å§‹ä¿å­˜æœ€ç»ˆæ¨¡å‹ ---")
try:
    final_model_path = os.path.join(output_dir, "test_model")  # ğŸ¯ ä¿å­˜åˆ°å­ç›®å½•
    trainer.save_model(final_model_path)
    print(f"--- âœ… å®Œæ•´æ¨¡å‹å·²ä¿å­˜è‡³: {final_model_path} ---")
except Exception as e:
    print(f"--- âŒ ä¿å­˜å‡ºé”™: {e} ---")
    raise

print("--- ğŸ‰ æµ‹è¯•å®Œæˆï¼---")
print("å¦‚æœä¸€åˆ‡æ­£å¸¸ï¼Œå¯ä»¥è°ƒæ•´ä»¥ä¸‹å‚æ•°è¿›è¡Œæ­£å¼è®­ç»ƒï¼š")
print("1. å¢åŠ  TEST_SAMPLES åˆ°æ›´å¤§çš„æ•°å€¼")
print("2. ç§»é™¤æˆ–å¢åŠ  max_steps é™åˆ¶")
print("3. è°ƒæ•´ gradient_accumulation_steps åˆ° 8")
print("4. è°ƒæ•´ save_steps åˆ° 200")