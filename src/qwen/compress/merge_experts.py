import torch
from torch import nn
from datetime import datetime

import numpy as np
import json
import os
from pathlib import Path
from typing import Dict, List, Tuple, Optional, cast
from transformers.models.qwen2_moe.modeling_qwen2_moe import Qwen2MoeSparseMoeBlock, Qwen2MoeMLP, Qwen2MoeForCausalLM, Qwen2MoeDecoderLayer
from transformers import PreTrainedTokenizerBase, Qwen2TokenizerFast, AutoModelForCausalLM, AutoTokenizer
import copy
from torch.linalg import svd

from dataclasses import dataclass, field

def load_clustering_results(cluster_dir: str, layer_idx: int) -> Tuple[torch.Tensor, Dict]:
    """åŠ è½½èšç±»ç»“æœ"""
    labels_path = os.path.join(cluster_dir, f"cluster_labels_layer_{layer_idx}.pt")
    info_path = os.path.join(cluster_dir, f"cluster_info_layer_{layer_idx}.json")
    
    if not os.path.exists(labels_path) or not os.path.exists(info_path):
        raise FileNotFoundError(f"Clustering results not found for layer {layer_idx}")

    cluster_labels: torch.Tensor = torch.load(labels_path, map_location='cpu')
    with open(info_path, 'r') as f:
        cluster_info: Dict = json.load(f)

    return cluster_labels, cluster_info

def load_activation_frequency(result_dir: str, layer_idx: int) -> torch.Tensor:
    """åŠ è½½ä¸“å®¶æ¿€æ´»é¢‘ç‡"""
    freq_path = os.path.join(result_dir, f"activation_frequency_layer_{layer_idx}.pt")
    
    if not os.path.exists(freq_path):
        raise FileNotFoundError(f"Activation frequency not found for layer {layer_idx}")
    
    freq_data = torch.load(freq_path, map_location='cpu')
    return freq_data['activation_counts']

def svd_subspace_alignment(expert_weights: List[torch.Tensor], relative_frequencies: List[float]) -> torch.Tensor:
    """
    ä½¿ç”¨SVDè¿›è¡Œå­ç©ºé—´å¯¹é½ - æŒ‰ç…§è®ºæ–‡å…¬å¼(5-7)
    
    Args:
        expert_weights: ä¸“å®¶æƒé‡çŸ©é˜µåˆ—è¡¨ï¼Œæ¯ä¸ªå½¢çŠ¶ä¸º [hidden_size, intermediate_size]
        relative_frequencies: å¯¹åº”çš„æ¿€æ´»é¢‘ç‡åˆ—è¡¨ï¼ˆå½’ä¸€åŒ–åï¼‰
        
    Returns:
        aligned_weight: å¯¹é½åçš„åˆå¹¶æƒé‡çŸ©é˜µ
    """
    if len(expert_weights) == 1:
        return expert_weights[0]
    
    if len(expert_weights) != len(relative_frequencies):
        raise ValueError("Number of weights and frequencies must match")

    # è®°å½•åŸå§‹æ•°æ®ç±»å‹
    original_dtype = expert_weights[0].dtype
    original_device = expert_weights[0].device

    # Step 1: è½¬æ¢ä¸ºfloat32è¿›è¡ŒSVDè®¡ç®—ï¼Œç¡®ä¿åœ¨åŒä¸€è®¾å¤‡ä¸Š
    expert_weights_float32 = [w.float().to(original_device) for w in expert_weights]
    
    # Step 2: å°†æ‰€æœ‰ä¸“å®¶æƒé‡å‚ç›´è¿æ¥ - æŒ‰ç…§è®ºæ–‡å…¬å¼(5)
    # W^(1), W^(2), ..., W^(n) -> [W^(1); W^(2); ...; W^(n)]
    concatenated_weights = torch.cat(expert_weights_float32, dim=1)  # [hidden_size, n_experts * intermediate_size]
    
    # Step 3: è¿›è¡ŒSVDåˆ†è§£: W = U Î£ V^T
    svd_result = svd(concatenated_weights, full_matrices=False)
    U: torch.Tensor = svd_result[0]  # [hidden_size, min(hidden_size, n_experts * intermediate_size)]
    S: torch.Tensor = svd_result[1]  # [min(hidden_size, n_experts * intermediate_size)]
    Vt: torch.Tensor = svd_result[2] # [min(hidden_size, n_experts * intermediate_size), n_experts * intermediate_size]
    
    # ä¿ç•™ä¸å•ä¸ªä¸“å®¶ç›¸åŒçš„ç»´åº¦
    intermediate_size = expert_weights[0].shape[1]  # intermediate_size
    
    
    # Step 4: å°†V^TçŸ©é˜µåˆ†å‰²å›åŸæ¥çš„ä¸“å®¶å—
    # V^Tçš„å½¢çŠ¶æ˜¯ [min(hidden_size, n_experts * intermediate_size), n_experts * intermediate_size]
    # æˆ‘ä»¬éœ€è¦å°†åˆ—ç»´åº¦åˆ†å‰²æˆ n_experts ä¸ªå—ï¼Œæ¯ä¸ªå—å¤§å°ä¸º intermediate_size
    V_blocks: List[torch.Tensor] = []
    for i in range(len(expert_weights)):
        start_col = i * intermediate_size
        end_col = (i + 1) * intermediate_size
        V_i = Vt[:, start_col:end_col]  # [min(hidden_size, n_experts * intermediate_size), intermediate_size]
        V_blocks.append(V_i)
    
    # Step 4: æŒ‰ç…§æ¿€æ´»é¢‘ç‡å¯¹Vå—è¿›è¡ŒåŠ æƒåˆå¹¶ - è®ºæ–‡å…¬å¼(7)
    # V_merged = Î£ f(V_i) * V_i / Î£ f(V_i)
    # åŠ æƒåˆå¹¶Vå—
    V_merged = torch.zeros_like(V_blocks[0])  # [svd_rank, intermediate_size]
    for V_block, weight in zip(V_blocks, relative_frequencies):
        V_merged += weight * V_block
    
    # Step 5: é‡æ„æœ€ç»ˆçš„æƒé‡çŸ©é˜µ
    # W_merged = U_reduced @ diag(S_reduced) @ V_merged^T
    aligned_weight = U @ torch.diag(S) @ V_merged  # [hidden_size, intermediate_size]
    
    # æ·»åŠ ç±»å‹è½¬æ¢
    aligned_weight = aligned_weight.to(dtype=original_dtype, device=original_device)

    return aligned_weight

def get_cluster_relative_frequencies(
    expert_frequencies: torch.Tensor,
    cluster_labels: torch.Tensor,
    cluster_id: int
) -> List[float]:
    """
    input: 
        expert_frequencies: ä¸“å®¶æ¿€æ´»é¢‘ç‡å¼ é‡
        cluster_labels: ä¸“å®¶èšç±»æ ‡ç­¾å¼ é‡
        cluster_id: å½“å‰èšç±»çš„ID
    output:
        relative_frequencies: èšç±»å†…ä¸“å®¶çš„ç›¸å¯¹æ¿€æ´»é¢‘ç‡
    è®¡ç®—èšç±»å†…ä¸“å®¶çš„ç›¸å¯¹æ¿€æ´»é¢‘ç‡
    """
    # æ‰¾åˆ°å±äºå½“å‰èšç±»çš„ä¸“å®¶
    expert_indices = torch.where(cluster_labels == cluster_id)[0]
    
    # è·å–èšç±»å†…ä¸“å®¶çš„æ¿€æ´»è®¡æ•°
    cluster_counts = expert_frequencies[expert_indices]
    
    # åœ¨èšç±»å†…é‡æ–°å½’ä¸€åŒ–
    total_cluster_counts = cluster_counts.sum().float()
    
    if total_cluster_counts > 0:
        # è®¡ç®—èšç±»å†…çš„ç›¸å¯¹é¢‘ç‡
        relative_frequencies = (cluster_counts.float() / total_cluster_counts).tolist()
    else:
        # å¦‚æœèšç±»å†…æ‰€æœ‰ä¸“å®¶æ¿€æ´»éƒ½ä¸º0ï¼Œä½¿ç”¨å‡åŒ€åˆ†å¸ƒ
        num_experts = len(expert_indices)
        relative_frequencies = [1.0 / num_experts] * num_experts
        print(f"Warning: All experts in cluster {cluster_id} have zero activation, using uniform distribution")
    
    return relative_frequencies

def merge_experts_in_moe_layer(
    moe_layer: Qwen2MoeSparseMoeBlock,
    cluster_labels: torch.Tensor,
    expert_frequencies: torch.Tensor,
    merging_method: str = "svd"
) -> Qwen2MoeSparseMoeBlock:
    """
    åˆå¹¶MoEå±‚ä¸­çš„ä¸“å®¶ - ä¿®å¤è®¾å¤‡ä¸€è‡´æ€§é—®é¢˜
    """
    # è·å–æ¨¡å‹æ‰€åœ¨çš„è®¾å¤‡
    model_device = next(moe_layer.parameters()).device

    # ç¡®ä¿æ‰€æœ‰å¼ é‡éƒ½åœ¨åŒä¸€è®¾å¤‡ä¸Š
    cluster_labels = cluster_labels.to(model_device)
    expert_frequencies = expert_frequencies.to(model_device)

    # åˆ›å»ºæ–°çš„MoEå±‚å‰¯æœ¬
    merged_moe_layer = copy.deepcopy(moe_layer)
    
    # è·å–èšç±»ä¿¡æ¯
    unique_clusters: List[int] = torch.unique(cluster_labels).tolist()
    n_merged_experts = len(unique_clusters)
    
    print(f"Merging {len(moe_layer.experts)} experts into {n_merged_experts} experts using {merging_method} method")
    
    # åˆ›å»ºæ–°çš„ä¸“å®¶åˆ—è¡¨
    new_experts: List[Qwen2MoeMLP] = []
    
    for cluster_id in unique_clusters:
        # æ‰¾åˆ°å±äºå½“å‰èšç±»çš„ä¸“å®¶
        expert_indices: List[int] = torch.where(cluster_labels == cluster_id)[0].tolist()
        
        if len(expert_indices) == 1:
            # å¦‚æœèšç±»ä¸­åªæœ‰ä¸€ä¸ªä¸“å®¶ï¼Œç›´æ¥å¤åˆ¶
            new_experts.append(cast(Qwen2MoeMLP, copy.deepcopy(moe_layer.experts[expert_indices[0]])))
            print(f"  Cluster {cluster_id}: single expert {expert_indices[0]}")
        else:
            # åˆå¹¶å¤šä¸ªä¸“å®¶
            print(f"  Cluster {cluster_id}: merging experts {expert_indices}")
            
            # æå–ä¸“å®¶æƒé‡
            cluster_experts = [moe_layer.experts[i] for i in expert_indices]
            
            # è®¡ç®—èšç±»å†…çš„ç›¸å¯¹é¢‘ç‡
            cluster_frequencies = get_cluster_relative_frequencies(
                expert_frequencies, cluster_labels, cluster_id
            )
            
            print(f"    Cluster relative frequencies: {[f'{f:.4f}' for f in cluster_frequencies]}")
            
            # åˆ›å»ºæ–°çš„ä¸“å®¶ä½œä¸ºæ¨¡æ¿
            merged_expert: Qwen2MoeMLP = cast(Qwen2MoeMLP, copy.deepcopy(cluster_experts[0]))
            
            # åˆ†åˆ«åˆå¹¶æ¯ä¸ªæƒé‡çŸ©é˜µ
            for param_name in ['gate_proj', 'up_proj', 'down_proj']:
                if hasattr(merged_expert, param_name):
                    # æ”¶é›†å½“å‰å‚æ•°çš„æƒé‡
                    param_weights: List[torch.Tensor] = []
                    for expert in cluster_experts:
                        param_weights.append(cast(nn.Linear,getattr(expert, param_name)).weight.data)
                    
                    # é€‰æ‹©åˆå¹¶æ–¹æ³•
                    merged_weight = svd_subspace_alignment(param_weights, cluster_frequencies)
                    
                    # æ›´æ–°åˆå¹¶åçš„æƒé‡
                    assert(merged_weight is not None)
                    cast(nn.Linear, getattr(merged_expert, param_name)).weight.data.copy_(merged_weight)
            
            new_experts.append(merged_expert)
    
    # æ›´æ–°MoEå±‚çš„ä¸“å®¶åˆ—è¡¨
    merged_moe_layer.experts = torch.nn.ModuleList(new_experts)
    merged_moe_layer.num_experts = len(new_experts)
    
    # æ›´æ–°è·¯ç”±å™¨çš„è¾“å‡ºç»´åº¦
    if hasattr(merged_moe_layer, 'gate'):
        old_gate_weight = merged_moe_layer.gate.weight.data # [num_experts, hidden_size]
        new_gate_weight = torch.zeros(n_merged_experts, old_gate_weight.shape[1])
        
        # ä¸ºæ¯ä¸ªæ–°ä¸“å®¶åˆ†é…è·¯ç”±æƒé‡
        for new_idx, cluster_id in enumerate(unique_clusters):
            expert_indices = torch.where(cluster_labels == cluster_id)[0].tolist()
            
            # ä½¿ç”¨æ¿€æ´»é¢‘ç‡ä½œä¸ºæƒé‡æ¥è®¡ç®—èšç±»çš„è·¯ç”±æƒé‡
            cluster_counts = expert_frequencies[expert_indices].float()
            total_cluster_counts = cluster_counts.sum()
            
            if total_cluster_counts > 0:
                # æŒ‰æ¿€æ´»é¢‘ç‡åŠ æƒå¹³å‡åŸå§‹è·¯ç”±æƒé‡
                weights = cluster_counts / total_cluster_counts # [len(expert_indices)]
                new_gate_weight[new_idx] = (old_gate_weight[expert_indices] * weights.unsqueeze(1)).sum(dim=0)
            else:
                # å¦‚æœæ²¡æœ‰æ¿€æ´»ï¼Œä½¿ç”¨ç®€å•å¹³å‡
                new_gate_weight[new_idx] = old_gate_weight[expert_indices].mean(dim=0)
        
        # é‡æ–°åˆ›å»ºgateå±‚
        merged_moe_layer.gate = torch.nn.Linear(
            old_gate_weight.shape[1], 
            n_merged_experts, 
            bias=False
        )
        merged_moe_layer.gate.weight.data.copy_(new_gate_weight)
    
    return merged_moe_layer

@dataclass
class MergeStats:
    total_layers_processed: int = 0
    total_experts_before: int = 0
    total_experts_after: int = 0
    layer_details: Dict[int, Dict] = field(default_factory=dict)

def merge_model_experts(
    model: Qwen2MoeForCausalLM,
    cluster_dir: str,
    result_dir: str,
    target_layers: List[int],
    merging_method: str = "svd"
) -> Qwen2MoeForCausalLM:
    """
    åˆå¹¶æ•´ä¸ªæ¨¡å‹ä¸­æŒ‡å®šå±‚çš„ä¸“å®¶
    
    Args:
        model: åŸå§‹æ¨¡å‹
        cluster_dir: èšç±»ç»“æœç›®å½•
        result_dir: åˆ†æç»“æœç›®å½•ï¼ˆç”¨äºåŠ è½½æ¿€æ´»é¢‘ç‡ï¼‰
        target_layers: è¦åˆå¹¶çš„å±‚åˆ—è¡¨
        merging_method: åˆå¹¶æ–¹æ³• ("svd" æˆ– "frequency")
        
    Returns:
        merged_model: åˆå¹¶åçš„æ¨¡å‹
    """
    print(f"Starting expert merging for {len(target_layers)} layers...")
    print(f"Target layers: {target_layers}")
    print(f"Merging method: {merging_method}")
    
    # åˆ›å»ºæ¨¡å‹å‰¯æœ¬
    merged_model = copy.deepcopy(model)
    
    # ç»Ÿè®¡ä¿¡æ¯
    merge_stats = MergeStats()
    
    for layer_idx in target_layers:
        print(f"\n{'='*50}")
        print(f"Processing layer {layer_idx}...")
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºMoEå±‚
        layer = cast(Qwen2MoeDecoderLayer, merged_model.model.layers[layer_idx])
        if not isinstance(layer.mlp, Qwen2MoeSparseMoeBlock):
            print(f"  âš ï¸  Layer {layer_idx} is not a MoE layer, skipping...")
            continue
        
        try:
            # è®°å½•åˆå¹¶å‰çš„ä¸“å®¶æ•°é‡
            experts_before = len(layer.mlp.experts)
            merge_stats.total_experts_before += experts_before
            
            # åŠ è½½èšç±»ç»“æœå’Œæ¿€æ´»é¢‘ç‡
            print(f"  ğŸ“‚ Loading clustering results for layer {layer_idx}...")
            cluster_labels, cluster_info = load_clustering_results(cluster_dir, layer_idx)
            
            print(f"  ğŸ“‚ Loading activation frequencies for layer {layer_idx}...")
            expert_frequencies = load_activation_frequency(result_dir, layer_idx)
            
            print(f"  ğŸ” Layer {layer_idx} info:")
            print(f"     - Experts before: {experts_before}")
            print(f"     - Target clusters: {cluster_info['n_clusters']}")
            print(f"     - Cluster sizes: {cluster_info['cluster_sizes']}")
            
            # åˆå¹¶ä¸“å®¶
            print(f"  ğŸ”„ Merging experts using {merging_method} method...")
            merged_moe = merge_experts_in_moe_layer(
                layer.mlp, 
                cluster_labels, 
                expert_frequencies,
                merging_method
            )
            
            # æ›¿æ¢å±‚
            layer.mlp = merged_moe
            
            # è®°å½•åˆå¹¶åçš„ä¸“å®¶æ•°é‡
            experts_after = len(merged_moe.experts)
            merge_stats.total_experts_after += experts_after
            merge_stats.total_layers_processed += 1
            
            # è®°å½•å±‚çº§è¯¦ç»†ä¿¡æ¯
            merge_stats.layer_details[layer_idx] = {
                'experts_before': experts_before,
                'experts_after': experts_after,
                'compression_ratio': experts_before / experts_after if experts_after > 0 else float('inf'),
                'cluster_sizes': cluster_info['cluster_sizes']
            }
            
            print(f"  âœ… Layer {layer_idx} merged successfully: {experts_before} -> {experts_after} experts")
            print(f"     Compression ratio: {experts_before/experts_after:.2f}x")
            
        except FileNotFoundError as e:
            print(f"  âŒ Error: Missing required files for layer {layer_idx}")
            print(f"     {str(e)}")
            continue
            
        except Exception as e:
            print(f"  âŒ Error processing layer {layer_idx}: {str(e)}")
            import traceback
            traceback.print_exc()
            continue
    
    # æ‰“å°æ€»ç»“ç»Ÿè®¡
    print(f"\n{'='*60}")
    print("ğŸ‰ EXPERT MERGING SUMMARY")
    print(f"{'='*60}")
    print(f"âœ… Successfully processed layers: {merge_stats.total_layers_processed}/{len(target_layers)}")
    print(f"ğŸ“Š Total experts before merging: {merge_stats.total_experts_before}")
    print(f"ğŸ“Š Total experts after merging: {merge_stats.total_experts_after}")

    if merge_stats.total_experts_before > 0:
        overall_compression = merge_stats.total_experts_before / merge_stats.total_experts_after
        print(f"ğŸ¯ Overall compression ratio: {overall_compression:.2f}x")
        print(f"ğŸ’¾ Model size reduction: {((merge_stats.total_experts_before - merge_stats.total_experts_after) / merge_stats.total_experts_before) * 100:.1f}%")

    print(f"\nPer-layer details:")
    for layer_idx, details in merge_stats.layer_details.items():
        print(f"  Layer {layer_idx}: {details['experts_before']} -> {details['experts_after']} experts ({details['compression_ratio']:.2f}x)")
    
    return merged_model


def save_merged_model(
    merged_model: Qwen2MoeForCausalLM,
    output_dir: str,
    model_name: str = "merged_model",
    save_config: bool = True,
    tokenizer: Optional[PreTrainedTokenizerBase] = None
) -> str:
    """
    ä¿å­˜åˆå¹¶åçš„æ¨¡å‹
    
    Args:
        merged_model: åˆå¹¶åçš„æ¨¡å‹
        output_dir: è¾“å‡ºç›®å½•
        model_name: æ¨¡å‹åç§°
        save_config: æ˜¯å¦ä¿å­˜é…ç½®ä¿¡æ¯
        tokenizer: åˆ†è¯å™¨ï¼ˆå¯é€‰ï¼Œå¦‚æœä¸æä¾›åˆ™ä¸ä¿å­˜åˆ†è¯å™¨ï¼‰
        
    Returns:
        model_path: ä¿å­˜çš„æ¨¡å‹è·¯å¾„
    """
    model_path = os.path.join(output_dir, model_name)
    os.makedirs(model_path, exist_ok=True)
    
    print(f"\n{'='*50}")
    print("ğŸ’¾ SAVING MERGED MODEL")
    print(f"{'='*50}")
    print(f"ğŸ“ Output directory: {model_path}")
    
    try:
        # ä¿å­˜æ¨¡å‹
        print("ğŸ”„ Saving model weights and configuration...")
        merged_model.save_pretrained(
            save_directory=model_path,
            safe_serialization=True,  # ä½¿ç”¨SafeTensorsæ ¼å¼
            max_shard_size="2GB"      # åˆ†ç‰‡å¤§å°
        )
        
        # ä¿å­˜åˆ†è¯å™¨ï¼ˆå¯é€‰ï¼‰
        if tokenizer is not None:
            print("ğŸ”„ Saving tokenizer...")
            try:
                tokenizer.save_pretrained(model_path)
                print("âœ… Tokenizer saved successfully!")
            except Exception as e:
                print(f"âš ï¸  Warning: Failed to save tokenizer: {e}")
                print("   Model can still be used with original tokenizer")
        else:
            print("â© Skipping tokenizer saving (not provided)")
            print("   ğŸ’¡ Tip: Use the original model's tokenizer when loading this merged model")
        
        # ä¿å­˜é¢å¤–çš„é…ç½®ä¿¡æ¯
        if save_config:
            print("ğŸ”„ Saving merge configuration...")
            merge_info = {
                "merge_timestamp": str(datetime.now()),
                "original_model_type": "Qwen2MoeForCausalLM",
                "merged_layers": [],  # è¿™ä¸ªå¯ä»¥åœ¨è°ƒç”¨æ—¶å¡«å……
                "merging_method": "svd",
                "total_parameters": sum(p.numel() for p in merged_model.parameters()),
                "trainable_parameters": sum(p.numel() for p in merged_model.parameters() if p.requires_grad),
                "moe_layers_info": {},
            }
            
            # ç»Ÿè®¡æ¯å±‚çš„ä¸“å®¶æ•°é‡
            moe_layer_info = {}
            for i, layer in enumerate(merged_model.model.layers):
                if isinstance(layer.mlp, Qwen2MoeSparseMoeBlock):
                    moe_layer_info[f"layer_{i}"] = {
                        "num_experts": len(layer.mlp.experts),
                        "is_moe_layer": True
                    }
                else:
                    moe_layer_info[f"layer_{i}"] = {
                        "is_moe_layer": False
                    }
            
            merge_info["moe_layers_info"] = moe_layer_info
            
            config_path = os.path.join(model_path, "merge_info.json")
            with open(config_path, 'w') as f:
                json.dump(merge_info, f, indent=2, default=str)
            
            print(f"ğŸ“ Merge configuration saved to: {config_path}")
        
        # éªŒè¯ä¿å­˜æ˜¯å¦æˆåŠŸ
        print("ğŸ” Validating saved model...")
        saved_files = os.listdir(model_path)
        required_files = ['config.json']
        
        missing_files = [f for f in required_files if f not in saved_files]
        if missing_files:
            print(f"âš ï¸  Warning: Missing files: {missing_files}")
        else:
            print("âœ… All required files saved successfully!")
        
        # è®¡ç®—æ¨¡å‹å¤§å°
        total_size = sum(os.path.getsize(os.path.join(model_path, f)) 
                        for f in saved_files if os.path.isfile(os.path.join(model_path, f)))
        size_gb = total_size / (1024**3)
        
        print(f"ğŸ“Š Model statistics:")
        print(f"   - Total parameters: {sum(p.numel() for p in merged_model.parameters()):,}")
        print(f"   - Model size on disk: {size_gb:.2f} GB")
        print(f"   - Number of files: {len(saved_files)}")
        
        print(f"âœ… Model successfully saved to: {model_path}")
        return model_path
        
    except Exception as e:
        print(f"âŒ Error saving model: {str(e)}")
        import traceback
        traceback.print_exc()
        raise

def main():
    """ä¸»å‡½æ•°ï¼šæ‰§è¡Œä¸“å®¶åˆå¹¶"""
    # é…ç½®å‚æ•°
    CLUSTER_N = 8  # èšç±»æ•°é‡
    MODEL_NAME = "Qwen/Qwen1.5-MoE-A2.7B"
    MODEL_PATH = "/root/fsas/models/Qwen/Qwen1.5-MoE-A2.7B"
    CLUSTER_DIR = f"/root/fsas/zhanghongyu/SMoE/qwen/analysis_results/kmeans_clusters_{CLUSTER_N}"  # èšç±»ç»“æœå­˜æ”¾ä½ç½®
    RESULT_DIR = "/root/fsas/zhanghongyu/SMoE/qwen/analysis_results/activation_frequency_results"   # æ¿€æ´»é¢‘ç‡å­˜æ”¾ä½ç½®
    OUTPUT_DIR = "/root/fsas/zhanghongyu/SMoE/qwen/merged_models"
    
    # è¦åˆå¹¶çš„MoEå±‚ (Qwen1.5-MoEçš„MoEå±‚é€šå¸¸æ˜¯å¥‡æ•°å±‚)
    TARGET_LAYERS = list(range(24))
    MERGING_METHOD = "svd"  # å¯é€‰: "svd" æˆ– "frequency"
    
    try:
        print("ğŸš€ Starting Expert Merging Pipeline")
        print(f"{'='*60}")
        
        # åŠ è½½åŸå§‹æ¨¡å‹ï¼ˆä¸åŠ è½½åˆ†è¯å™¨ï¼Œé¿å…æ½œåœ¨é—®é¢˜ï¼‰
        print("ğŸ“‚ Loading original model...")
        
        model = cast(Qwen2MoeForCausalLM, AutoModelForCausalLM.from_pretrained(
            MODEL_NAME,
            cache_dir = MODEL_PATH,
            dtype=torch.bfloat16,
            device_map="auto", 
            trust_remote_code=True
        ))

        print(f"âœ… Model loaded: {type(model).__name__}")
        
        # æ‰§è¡Œä¸“å®¶åˆå¹¶
        merged_model = merge_model_experts(
            model=model,
            cluster_dir=CLUSTER_DIR,
            result_dir=RESULT_DIR,
            target_layers=TARGET_LAYERS,
            merging_method=MERGING_METHOD
        )
        
        # ä¿å­˜åˆå¹¶åçš„æ¨¡å‹ï¼ˆä¸ä¿å­˜åˆ†è¯å™¨ï¼‰
        model_name = f"qwen1.5_moe_merged_{MERGING_METHOD}_CLUSTER_{CLUSTER_N}"
        saved_path = save_merged_model(
            merged_model=merged_model,
            output_dir=OUTPUT_DIR,
            model_name=model_name,
            save_config=True,
            tokenizer=None  # ä¸ä¿å­˜åˆ†è¯å™¨
        )
        
        print(f"\nğŸ‰ Expert merging pipeline completed successfully!")
        print(f"ğŸ¯ Merged model saved to: {saved_path}")
        print(f"ğŸ“ Note: Tokenizer not saved. Use original model's tokenizer when loading.")
        
    except Exception as e:
        print(f"ğŸ’¥ Fatal error during merging: {e}")
        import traceback
        traceback.print_exc()
        raise

if __name__ == "__main__":
    main()