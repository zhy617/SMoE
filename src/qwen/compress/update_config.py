import json
import os
import glob
import argparse
from typing import Dict, List

def scan_cluster_results(cluster_dir: str) -> Dict[int, int]:
    """
    æ‰«æèšç±»ç»“æœç›®å½•ï¼Œä»æ¯ä¸ªJSONæ–‡ä»¶ä¸­æå–æ¯å±‚çš„ä¸“å®¶æ•°é‡ã€‚

    Args:
        cluster_dir: åŒ…å« `cluster_info_layer_*.json` æ–‡ä»¶çš„ç›®å½•ã€‚

    Returns:
        ä¸€ä¸ªå­—å…¸ï¼Œå°†å±‚ç´¢å¼•æ˜ å°„åˆ°è¯¥å±‚çš„ä¸“å®¶æ•°é‡ã€‚
    """
    print(f"ğŸ” æ­£åœ¨æ‰«æç›®å½•: {cluster_dir}")
    layer_expert_counts: Dict[int, int] = {}
    
    # ä½¿ç”¨globæŸ¥æ‰¾æ‰€æœ‰ç¬¦åˆæ¨¡å¼çš„JSONæ–‡ä»¶
    json_files = glob.glob(os.path.join(cluster_dir, "cluster_info_layer_*.json"))

    if not json_files:
        raise FileNotFoundError(f"åœ¨ '{cluster_dir}' ä¸­æ²¡æœ‰æ‰¾åˆ° 'cluster_info_layer_*.json' æ–‡ä»¶ã€‚")

    print(f"   æ‰¾åˆ°äº† {len(json_files)} ä¸ªèšç±»ç»“æœæ–‡ä»¶ã€‚")

    for file_path in json_files:
        try:
            # ä»æ–‡ä»¶åä¸­æå–å±‚ç´¢å¼•
            # ä¾‹å¦‚: '.../cluster_info_layer_0.json' -> 0
            filename = os.path.basename(file_path)
            layer_idx_str = filename.split('_')[-1].split('.')[0]
            layer_idx = int(layer_idx_str)

            # è¯»å–JSONæ–‡ä»¶å¹¶æå–n_clusters
            with open(file_path, 'r') as f:
                data = json.load(f)
                n_clusters = data['n_clusters']
                layer_expert_counts[layer_idx] = n_clusters
                print(f"   - ç¬¬ {layer_idx:2d} å±‚: {n_clusters:2d} ä¸ªä¸“å®¶")

        except (ValueError, KeyError, IndexError) as e:
            print(f"âš ï¸  å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}ã€‚å·²è·³è¿‡ã€‚")
            continue
            
    if not layer_expert_counts:
        raise ValueError("æœªèƒ½ä»ä»»ä½•æ–‡ä»¶ä¸­æˆåŠŸæå–ä¸“å®¶æ•°é‡ã€‚")

    return layer_expert_counts

def update_config_file(
    config_path: str, 
    layer_expert_counts: Dict[int, int], 
    num_layers: int,
    backup: bool = True
) -> None:
    """
    ä½¿ç”¨æ¯å±‚çš„ä¸“å®¶æ•°é‡æ›´æ–°æ¨¡å‹çš„config.jsonæ–‡ä»¶ã€‚

    Args:
        config_path: `config.json` æ–‡ä»¶çš„è·¯å¾„ã€‚
        layer_expert_counts: å±‚ç´¢å¼•åˆ°ä¸“å®¶æ•°é‡çš„æ˜ å°„ã€‚
        num_layers: æ¨¡å‹é¢„æœŸçš„æ€»å±‚æ•°ã€‚
        backup: æ˜¯å¦åˆ›å»ºåŸå§‹é…ç½®æ–‡ä»¶çš„å¤‡ä»½ã€‚
    """
    if not os.path.exists(config_path):
        raise FileNotFoundError(f"é…ç½®æ–‡ä»¶æœªæ‰¾åˆ°: {config_path}")

    # 1. å°†å­—å…¸è½¬æ¢ä¸ºæœ‰åºåˆ—è¡¨ï¼Œå¹¶æ£€æŸ¥æ˜¯å¦æœ‰ç¼ºå¤±çš„å±‚
    expert_counts_list: List[int] = []
    for i in range(num_layers):
        if i not in layer_expert_counts:
            raise ValueError(f"ç¬¬ {i} å±‚çš„èšç±»ç»“æœç¼ºå¤±ï¼Œæ— æ³•ç»§ç»­ã€‚")
        expert_counts_list.append(layer_expert_counts[i])

    print(f"\nâœ… å·²ä¸ºæ‰€æœ‰ {num_layers} å±‚å‡†å¤‡å¥½ä¸“å®¶æ•°é‡åˆ—è¡¨ã€‚")

    backup_path = ""

    # 2. åˆ›å»ºå¤‡ä»½
    if backup:
        backup_path = config_path + ".bak"
        print(f"ğŸ’¾ æ­£åœ¨åˆ›å»ºå¤‡ä»½: {backup_path}")
        os.rename(config_path, backup_path)

    # 3. è¯»å–é…ç½®æ–‡ä»¶ï¼ˆä»å¤‡ä»½ä¸­è¯»å–ï¼‰
    try:
        
        with open(backup_path if backup else config_path, 'r') as f:
            config_data = json.load(f)

        # 4. æ·»åŠ æˆ–æ›´æ–° `layer_expert_counts` å­—æ®µ
        config_data['layer_expert_counts'] = expert_counts_list
        
        # (å¯é€‰) æ·»åŠ ä¸€äº›å…ƒæ•°æ®
        total_experts = sum(expert_counts_list)
        avg_experts = total_experts / num_layers
        config_data['_expert_statistics'] = {
            "total_experts": total_experts,
            "average_experts_per_layer": round(avg_experts, 2),
            "min_experts": min(expert_counts_list),
            "max_experts": max(expert_counts_list),
        }


        # 5. å†™å›æ–°çš„é…ç½®æ–‡ä»¶
        with open(config_path, 'w') as f:
            json.dump(config_data, f, indent=2)

        print(f"ğŸš€ é…ç½®æ–‡ä»¶ '{config_path}' æ›´æ–°æˆåŠŸï¼")
        print(f"   - æ–°å¢å­—æ®µ 'layer_expert_counts': {expert_counts_list}")
        print(f"   - æ€»ä¸“å®¶æ•°: {total_experts}, å¹³å‡æ¯å±‚ä¸“å®¶æ•°: {avg_experts:.2f}")

    except Exception as e:
        print(f"âŒ æ›´æ–°é…ç½®æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        # å¦‚æœå‡ºé”™ï¼Œæ¢å¤å¤‡ä»½
        if backup:
            print(f"ğŸ”„ æ­£åœ¨ä»å¤‡ä»½æ¢å¤...")
            os.rename(backup_path, config_path)
        raise

def main():
    parser = argparse.ArgumentParser(
        description="æ ¹æ®èšç±»ç»“æœæ›´æ–°æ¨¡å‹çš„config.jsonï¼Œä»¥æ”¯æŒæ¯å±‚ä¸åŒçš„ä¸“å®¶æ•°é‡ã€‚"
    )
    parser.add_argument(
        "--cluster_dir",
        type=str,
        required=True,
        help="åŒ…å« `cluster_info_layer_*.json` æ–‡ä»¶çš„ç›®å½•è·¯å¾„ã€‚"
    )
    parser.add_argument(
        "--config_path",
        type=str,
        required=True,
        help="è¦æ›´æ–°æ¨¡å‹è·¯å¾„"
    )
    parser.add_argument(
        "--num_layers",
        type=int,
        default=24,
        help="æ¨¡å‹é¢„æœŸçš„æ€»å±‚æ•° (ä¾‹å¦‚ Qwen1.5-MoE-A2.7B ä¸º 24)ã€‚"
    )
    parser.add_argument(
        "--no_backup",
        action="store_true",
        help="ä¸åˆ›å»ºåŸå§‹é…ç½®æ–‡ä»¶çš„å¤‡ä»½ã€‚"
    )
    parser.add_argument(
        "--CLUSTER_N",
        type=int, default=30,
        help="èšç±»æ—¶çš„ä¸“å®¶æ•°é‡ (ä»…ç”¨äºæ—¥å¿—æ˜¾ç¤º)ã€‚"
    )
    
    args = parser.parse_args()

    try:
        # path é‡Œæ·»åŠ èšç±»æ•°é‡
        cluster_dir = os.path.join(args.cluster_dir, f"kmeans_clusters_{args.CLUSTER_N}")

        config_path = os.path.join(args.config_path, f"qwen1.5_moe_merged_svd_cluster_{args.CLUSTER_N}/config.json")

        # æ­¥éª¤ 1: æ‰«æå¹¶è·å–æ¯å±‚çš„ä¸“å®¶æ•°
        layer_counts_dict = scan_cluster_results(cluster_dir)

        # æ­¥éª¤ 2: æ›´æ–°é…ç½®æ–‡ä»¶
        update_config_file(
            config_path=config_path,
            layer_expert_counts=layer_counts_dict,
            num_layers=args.num_layers,
            backup=not args.no_backup
        )
        print("\nğŸ‰ æ“ä½œå®Œæˆï¼")

    except (FileNotFoundError, ValueError) as e:
        print(f"\nğŸ’¥ é”™è¯¯: {e}")
        exit(1)

if __name__ == "__main__":
    main()