import os
import torch
import json
from tqdm import tqdm
import argparse
import numpy as np
from typing import cast, Dict

# é…ç½®è·¯å¾„
SIMILARITY_RESULTS_DIR = "/root/fsas/zhanghongyu/SMoE/qwen/analysis_results/similarity_results"
REDUNDANCY_OUTPUT_DIR = "/root/fsas/zhanghongyu/SMoE/qwen/analysis_results/redundancy_results"
NUM_LAYERS = 24   # Qwen1.5-MoE-A2.7B çš„å±‚æ•°


def calculate_layer_redundancy(similarity_matrix: torch.Tensor) -> float:
    """
    è®¡ç®—å•å±‚çš„å†—ä½™åº¦ï¼šç›¸ä¼¼åº¦çŸ©é˜µä¸­æ¯å¯¹ä¸“å®¶çš„ç›¸ä¼¼åº¦å¹³å‡å€¼
    
    Args:
        similarity_matrix: ä¸“å®¶ç›¸ä¼¼åº¦çŸ©é˜µ (num_experts, num_experts)
        
    Returns:
        å†—ä½™åº¦ï¼ˆå¹³å‡ç›¸ä¼¼åº¦ï¼Œæ’é™¤å¯¹è§’çº¿ï¼‰
    """
    num_experts = similarity_matrix.shape[0]
    
    # æ’é™¤å¯¹è§’çº¿å…ƒç´ ï¼Œè®¡ç®—æ‰€æœ‰ä¸“å®¶å¯¹çš„å¹³å‡ç›¸ä¼¼åº¦
    mask = ~torch.eye(num_experts, dtype=torch.bool)
    off_diagonal_similarities = similarity_matrix[mask]
    redundancy = off_diagonal_similarities.mean().item()
    
    return redundancy


def allocate_experts_by_redundancy(layer_redundancy: dict, group_size: int, target_avg_experts: int, 
                                 min_experts: int = 5, max_experts: int = 55) -> dict:
    """
    åŸºäºå†—ä½™åº¦ä¸ºç›¸é‚»nå±‚åˆ†é…ä¸“å®¶æ•°é‡
    
    Args:
        layer_redundancy: æ¯å±‚çš„å†—ä½™åº¦å­—å…¸ {layer_idx: redundancy_value}
        group_size: ç›¸é‚»å±‚çš„ç»„å¤§å° n
        target_avg_experts: ç›®æ ‡å¹³å‡ä¸“å®¶æ•°
        min_experts: æœ€å°‘ä¸“å®¶æ•°
        max_experts: æœ€å¤šä¸“å®¶æ•°
        
    Returns:
        æ¯å±‚çš„ä¸“å®¶åˆ†é… {layer_idx: num_experts}
    """
    print(f"\nğŸ¯ å¼€å§‹åŸºäºå†—ä½™åº¦åˆ†é…ä¸“å®¶æ•°é‡...")
    print(f"   - ç»„å¤§å°: {group_size} å±‚")
    print(f"   - ç›®æ ‡å¹³å‡ä¸“å®¶æ•°: {target_avg_experts}")
    print(f"   - ä¸“å®¶æ•°èŒƒå›´: [{min_experts}, {max_experts}]")
    
    expert_allocation: Dict[int, int] = {}
    
    # æŒ‰ç»„å¤„ç†å±‚
    num_groups = (NUM_LAYERS + group_size - 1) // group_size  # å‘ä¸Šå–æ•´
    
    for group_idx in range(num_groups):
        start_layer = group_idx * group_size
        end_layer = min(start_layer + group_size, NUM_LAYERS)
        
        print(f"\nğŸ“Š å¤„ç†ç¬¬ {group_idx + 1} ç»„: ç¬¬ {start_layer} - {end_layer - 1} å±‚")
        
        # è·å–è¯¥ç»„çš„å†—ä½™åº¦å€¼
        group_layers = list(range(start_layer, end_layer))
        group_redundancies = []
        
        for layer_idx in group_layers:
            if layer_idx in layer_redundancy:
                group_redundancies.append(layer_redundancy[layer_idx])
            else:
                print(f"   âš ï¸  ç¬¬ {layer_idx} å±‚æ²¡æœ‰å†—ä½™åº¦æ•°æ®ï¼Œä½¿ç”¨å¹³å‡å€¼")
                # å¦‚æœæŸå±‚ç¼ºå¤±ï¼Œä½¿ç”¨å·²æœ‰å±‚çš„å¹³å‡å€¼
                if group_redundancies:
                    group_redundancies.append(sum(group_redundancies) / len(group_redundancies))
                else:
                    group_redundancies.append(0.8)  # é»˜è®¤å€¼
        
        # è®¡ç®—è¯¥ç»„çš„æ€»ä¸“å®¶é¢„ç®—
        group_target_total = target_avg_experts * len(group_layers)
        
        # åŸºäºå†—ä½™åº¦åˆ†é…ï¼šå†—ä½™åº¦é«˜çš„å±‚åˆ†é…è¾ƒå°‘ä¸“å®¶ï¼Œå†—ä½™åº¦ä½çš„å±‚åˆ†é…è¾ƒå¤šä¸“å®¶
        # ä½¿ç”¨åæ¯”ä¾‹åˆ†é…ï¼šä¸“å®¶æ•° âˆ 1 / å†—ä½™åº¦
        inverse_redundancies = [1.0 / max(r, 0.00000001) for r in group_redundancies]  # é¿å…é™¤é›¶
        total_inverse: float = cast(float, sum(inverse_redundancies))
        
        # æŒ‰æ¯”ä¾‹åˆ†é…ä¸“å®¶æ•°
        raw_allocations = [(inv_r / total_inverse) * group_target_total for inv_r in inverse_redundancies]
        
        # çº¦æŸåˆ°èŒƒå›´å¹¶å–æ•´
        constrained_allocations = [max(min_experts, min(max_experts, round(alloc))) for alloc in raw_allocations]
        
        # è°ƒæ•´æ€»æ•°ä»¥åŒ¹é…ç›®æ ‡
        current_total = sum(constrained_allocations)
        target_total = round(group_target_total)
        diff = target_total - current_total
        
        # å¦‚æœæœ‰å·®å¼‚ï¼Œä¼˜å…ˆè°ƒæ•´å†—ä½™åº¦æœ€é«˜/æœ€ä½çš„å±‚
        if diff != 0:
            # æŒ‰å†—ä½™åº¦æ’åºï¼Œå‡†å¤‡è°ƒæ•´
            sorted_indices = sorted(range(len(group_redundancies)), 
                                  key=lambda i: group_redundancies[i], 
                                  reverse=(diff < 0))  # diff<0æ—¶ä»é«˜å†—ä½™åº¦å¼€å§‹å‡å°‘
            
            # ä¿®å¤åçš„é€»è¾‘ï¼šç¡®ä¿èƒ½å¤Ÿå®Œå…¨æ¶ˆé™¤å·®å¼‚
            remaining_diff = diff
            attempts = 0
            max_attempts = abs(diff) * len(sorted_indices)  # é¿å…æ— é™å¾ªç¯
            
            while remaining_diff != 0 and attempts < max_attempts:
                adjusted = False  # æ ‡è®°æœ¬è½®æ˜¯å¦æœ‰è°ƒæ•´
                
                for idx in sorted_indices:
                    if remaining_diff == 0:
                        break
                        
                    if remaining_diff > 0:  # éœ€è¦å¢åŠ ä¸“å®¶
                        if constrained_allocations[idx] < max_experts:
                            constrained_allocations[idx] += 1
                            remaining_diff -= 1
                            adjusted = True
                    else:  # éœ€è¦å‡å°‘ä¸“å®¶ (remaining_diff < 0)
                        if constrained_allocations[idx] > min_experts:
                            constrained_allocations[idx] -= 1
                            remaining_diff += 1
                            adjusted = True
                
                # å¦‚æœæœ¬è½®æ²¡æœ‰ä»»ä½•è°ƒæ•´ï¼Œè¯´æ˜æ— æ³•è¿›ä¸€æ­¥è°ƒæ•´ï¼Œè·³å‡ºå¾ªç¯
                if not adjusted:
                    break
                    
                attempts += 1
            
            # å¦‚æœè¿˜æœ‰å‰©ä½™å·®å¼‚ï¼Œç»™å‡ºè­¦å‘Š
            if remaining_diff != 0:
                print(f"   âš ï¸  è­¦å‘Š: æ— æ³•å®Œå…¨æ¶ˆé™¤å·®å¼‚ï¼Œå‰©ä½™å·®å¼‚={remaining_diff}")
        
        # è®°å½•åˆ†é…ç»“æœ
        for i, layer_idx in enumerate(group_layers):
            expert_allocation[layer_idx] = constrained_allocations[i]
            print(f"   ç¬¬ {layer_idx:2d} å±‚: å†—ä½™åº¦={group_redundancies[i]:.4f} -> ä¸“å®¶æ•°={constrained_allocations[i]}")
        
        group_actual_total = sum(constrained_allocations)
        group_actual_avg = group_actual_total / len(group_layers)
        print(f"   ç»„æ±‡æ€»: æ€»ä¸“å®¶æ•°={group_actual_total}, å®é™…å¹³å‡={group_actual_avg:.2f}")
    
    return expert_allocation


def main():
    """
    ä¸»å‡½æ•°ï¼šè®¡ç®—æ‰€æœ‰å±‚çš„å†—ä½™åº¦å¹¶åŸºäºå†—ä½™åº¦åˆ†é…ä¸“å®¶æ•°é‡
    """
    parser = argparse.ArgumentParser(description="è®¡ç®—ä¸“å®¶å†—ä½™åº¦å¹¶åˆ†é…ä¸“å®¶æ•°é‡")
    parser.add_argument("--group_size", type=int, default=4, help="ç›¸é‚»å±‚ç»„çš„å¤§å°")
    parser.add_argument("--target_avg_CLUSTER_N", type=float, default=30.0, help="ç›®æ ‡å¹³å‡ä¸“å®¶æ•°")
    parser.add_argument("--min_experts", type=int, default=15, help="æœ€å°‘ä¸“å®¶æ•°")
    parser.add_argument("--max_experts", type=int, default=45, help="æœ€å¤šä¸“å®¶æ•°")
    parser.add_argument("--allocate_experts", action="store_true", help="æ˜¯å¦è¿›è¡Œä¸“å®¶åˆ†é…")
    
    args = parser.parse_args()
    
    print("ğŸš€ å¼€å§‹è®¡ç®—ä¸“å®¶å†—ä½™åº¦...")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(REDUNDANCY_OUTPUT_DIR, exist_ok=True)
    
    # å­˜å‚¨ç»“æœ
    layer_redundancy = {}
    
    # éå†æ‰€æœ‰å±‚
    for layer_idx in tqdm(range(NUM_LAYERS), desc="è®¡ç®—å„å±‚å†—ä½™åº¦"):
        similarity_file = os.path.join(SIMILARITY_RESULTS_DIR, f"avg_similarity_matrix_layer_{layer_idx}.pt")
        
        if not os.path.exists(similarity_file):
            print(f"âš ï¸  è­¦å‘Š: æœªæ‰¾åˆ°ç¬¬ {layer_idx} å±‚çš„ç›¸ä¼¼åº¦çŸ©é˜µæ–‡ä»¶")
            continue
        
        # åŠ è½½ç›¸ä¼¼åº¦çŸ©é˜µå¹¶è®¡ç®—å†—ä½™åº¦
        try:
            similarity_matrix = torch.load(similarity_file)
            redundancy = calculate_layer_redundancy(similarity_matrix)
            layer_redundancy[layer_idx] = redundancy
            
            print(f"ç¬¬ {layer_idx:2d} å±‚å†—ä½™åº¦: {redundancy:.4f}")
            
        except Exception as e:
            print(f"âŒ å¤„ç†ç¬¬ {layer_idx} å±‚æ—¶å‡ºé”™: {e}")
            continue
    
    # ä¿å­˜å†—ä½™åº¦ç»“æœ
    results_file = os.path.join(REDUNDANCY_OUTPUT_DIR, "layer_redundancy.json")
    with open(results_file, 'w') as f:
        json.dump(layer_redundancy, f, indent=2)
    
    print(f"\nğŸ’¾ å†—ä½™åº¦ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
    
    # è¾“å‡ºæ±‡æ€»ä¿¡æ¯
    if layer_redundancy:
        avg_redundancy = sum(layer_redundancy.values()) / len(layer_redundancy)
        print(f"ğŸ“Š å¹³å‡å†—ä½™åº¦: {avg_redundancy:.4f}")
        print(f"ğŸ“Š å¤„ç†å±‚æ•°: {len(layer_redundancy)}/{NUM_LAYERS}")
    
    # å¦‚æœå¯ç”¨äº†ä¸“å®¶åˆ†é…åŠŸèƒ½
    if args.allocate_experts and layer_redundancy:
        expert_allocation = allocate_experts_by_redundancy(
            layer_redundancy, 
            args.group_size, 
            args.target_avg_CLUSTER_N,
            args.min_experts,
            args.max_experts
        )
        
        # ä¿å­˜ä¸“å®¶åˆ†é…ç»“æœ
        allocation_file = os.path.join(REDUNDANCY_OUTPUT_DIR, 
                                     f"expert_allocation_group{args.group_size}_avg{args.target_avg_CLUSTER_N}.json")
        with open(allocation_file, 'w') as f:
            json.dump({
                "config": {
                    "group_size": args.group_size,
                    "target_avg_cluster_n": args.target_avg_CLUSTER_N,
                    "min_experts": args.min_experts,
                    "max_experts": args.max_experts
                },
                "allocation": expert_allocation,
                "statistics": {
                    "total_experts": sum(expert_allocation.values()),
                    "actual_avg_experts": sum(expert_allocation.values()) / len(expert_allocation),
                    "min_allocated": min(expert_allocation.values()),
                    "max_allocated": max(expert_allocation.values())
                }
            }, f, indent=2)
        
        print(f"\nğŸ’¾ ä¸“å®¶åˆ†é…ç»“æœå·²ä¿å­˜åˆ°: {allocation_file}")
        
        # è¾“å‡ºåˆ†é…ç»Ÿè®¡
        total_experts = sum(expert_allocation.values())
        actual_avg = total_experts / len(expert_allocation)
        print(f"ğŸ¯ åˆ†é…ç»Ÿè®¡:")
        print(f"   - æ€»ä¸“å®¶æ•°: {total_experts}")
        print(f"   - å®é™…å¹³å‡ä¸“å®¶æ•°: {actual_avg:.2f}")
        print(f"   - æœ€å°‘åˆ†é…: {min(expert_allocation.values())}")
        print(f"   - æœ€å¤šåˆ†é…: {max(expert_allocation.values())}")
    
    print("ğŸ‰ åˆ†æå®Œæˆ!")


if __name__ == "__main__":
    main()