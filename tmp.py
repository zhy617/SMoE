# ...existing code...

from transformers import Qwen2MoeForCausalLM

def merge_model_experts(
    model: Qwen2MoeForCausalLM,
    cluster_dir: str,
    result_dir: str,
    target_layers: List[int],
    merging_method: str = "svd"
) -> Qwen2MoeForCausalLM:
    """
    åˆå¹¶æ•´ä¸ªæ¨¡å‹ä¸­æŒ‡å®šå±‚çš„ä¸“å®¶
    
    Args:
        model: åŸå§‹æ¨¡å‹
        cluster_dir: èšç±»ç»“æœç›®å½•
        result_dir: åˆ†æç»“æœç›®å½•ï¼ˆç”¨äºåŠ è½½æ¿€æ´»é¢‘ç‡ï¼‰
        target_layers: è¦åˆå¹¶çš„å±‚åˆ—è¡¨
        merging_method: åˆå¹¶æ–¹æ³• ("svd" æˆ– "frequency")
        
    Returns:
        merged_model: åˆå¹¶åçš„æ¨¡å‹
    """
    print(f"Starting expert merging for {len(target_layers)} layers...")
    print(f"Target layers: {target_layers}")
    print(f"Merging method: {merging_method}")
    
    # åˆ›å»ºæ¨¡å‹å‰¯æœ¬
    merged_model = copy.deepcopy(model)
    
    # ç»Ÿè®¡ä¿¡æ¯
    merge_stats = {
        'total_layers_processed': 0,
        'total_experts_before': 0,
        'total_experts_after': 0,
        'layer_details': {}
    }
    
    for layer_idx in target_layers:
        print(f"\n{'='*50}")
        print(f"Processing layer {layer_idx}...")
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºMoEå±‚
        layer = merged_model.model.layers[layer_idx]
        if not isinstance(layer.mlp, Qwen2MoeSparseMoeBlock):
            print(f"  âš ï¸  Layer {layer_idx} is not a MoE layer, skipping...")
            continue
        
        try:
            # è®°å½•åˆå¹¶å‰çš„ä¸“å®¶æ•°é‡
            experts_before = len(layer.mlp.experts)
            merge_stats['total_experts_before'] += experts_before
            
            # åŠ è½½èšç±»ç»“æœå’Œæ¿€æ´»é¢‘ç‡
            print(f"  ğŸ“‚ Loading clustering results for layer {layer_idx}...")
            cluster_labels, cluster_info = load_clustering_results(cluster_dir, layer_idx)
            
            print(f"  ğŸ“‚ Loading activation frequencies for layer {layer_idx}...")
            expert_frequencies = load_activation_frequency(result_dir, layer_idx)
            
            print(f"  ğŸ” Layer {layer_idx} info:")
            print(f"     - Experts before: {experts_before}")
            print(f"     - Target clusters: {cluster_info['n_clusters']}")
            print(f"     - Cluster sizes: {cluster_info['cluster_sizes']}")
            
            # åˆå¹¶ä¸“å®¶
            print(f"  ğŸ”„ Merging experts using {merging_method} method...")
            merged_moe = merge_experts_in_moe_layer(
                layer.mlp, 
                cluster_labels, 
                expert_frequencies,
                merging_method
            )
            
            # æ›¿æ¢å±‚
            layer.mlp = merged_moe
            
            # è®°å½•åˆå¹¶åçš„ä¸“å®¶æ•°é‡
            experts_after = len(merged_moe.experts)
            merge_stats['total_experts_after'] += experts_after
            merge_stats['total_layers_processed'] += 1
            
            # è®°å½•å±‚çº§è¯¦ç»†ä¿¡æ¯
            merge_stats['layer_details'][layer_idx] = {
                'experts_before': experts_before,
                'experts_after': experts_after,
                'compression_ratio': experts_before / experts_after if experts_after > 0 else float('inf'),
                'cluster_sizes': cluster_info['cluster_sizes']
            }
            
            print(f"  âœ… Layer {layer_idx} merged successfully: {experts_before} -> {experts_after} experts")
            print(f"     Compression ratio: {experts_before/experts_after:.2f}x")
            
        except FileNotFoundError as e:
            print(f"  âŒ Error: Missing required files for layer {layer_idx}")
            print(f"     {str(e)}")
            continue
            
        except Exception as e:
            print(f"  âŒ Error processing layer {layer_idx}: {str(e)}")
            import traceback
            traceback.print_exc()
            continue
    
    # æ‰“å°æ€»ç»“ç»Ÿè®¡
    print(f"\n{'='*60}")
    print("ğŸ‰ EXPERT MERGING SUMMARY")
    print(f"{'='*60}")
    print(f"âœ… Successfully processed layers: {merge_stats['total_layers_processed']}/{len(target_layers)}")
    print(f"ğŸ“Š Total experts before merging: {merge_stats['total_experts_before']}")
    print(f"ğŸ“Š Total experts after merging: {merge_stats['total_experts_after']}")
    
    if merge_stats['total_experts_before'] > 0:
        overall_compression = merge_stats['total_experts_before'] / merge_stats['total_experts_after']
        print(f"ğŸ¯ Overall compression ratio: {overall_compression:.2f}x")
        print(f"ğŸ’¾ Model size reduction: {((merge_stats['total_experts_before'] - merge_stats['total_experts_after']) / merge_stats['total_experts_before']) * 100:.1f}%")
    
    print(f"\nPer-layer details:")
    for layer_idx, details in merge_stats['layer_details'].items():
        print(f"  Layer {layer_idx}: {details['experts_before']} -> {details['experts_after']} experts ({details['compression_ratio']:.2f}x)")
    
    return merged_model

def save_merged_model(
    merged_model: Qwen2MoeForCausalLM,
    tokenizer,  # æ·»åŠ åˆ†è¯å™¨å‚æ•°
    output_dir: str,
    model_name: str = "merged_model",
    save_config: bool = True
) -> str:
    """
    ä¿å­˜åˆå¹¶åçš„æ¨¡å‹
    
    Args:
        merged_model: åˆå¹¶åçš„æ¨¡å‹
        tokenizer: åˆ†è¯å™¨
        output_dir: è¾“å‡ºç›®å½•
        model_name: æ¨¡å‹åç§°
        save_config: æ˜¯å¦ä¿å­˜é…ç½®ä¿¡æ¯
        
    Returns:
        model_path: ä¿å­˜çš„æ¨¡å‹è·¯å¾„
    """
    model_path = os.path.join(output_dir, model_name)
    os.makedirs(model_path, exist_ok=True)
    
    print(f"\n{'='*50}")
    print("ğŸ’¾ SAVING MERGED MODEL")
    print(f"{'='*50}")
    print(f"ğŸ“ Output directory: {model_path}")
    
    try:
        # ä¿å­˜æ¨¡å‹
        print("ğŸ”„ Saving model weights and configuration...")
        merged_model.save_pretrained(
            model_path,
            safe_serialization=True,  # ä½¿ç”¨SafeTensorsæ ¼å¼
            max_shard_size="2GB"      # åˆ†ç‰‡å¤§å°
        )
        
        # ä¿å­˜åˆ†è¯å™¨
        if tokenizer is not None:
            print("ğŸ”„ Saving tokenizer...")
            tokenizer.save_pretrained(model_path)
        
        # ä¿å­˜é¢å¤–çš„é…ç½®ä¿¡æ¯
        if save_config:
            print("ğŸ”„ Saving merge configuration...")
            merge_info = {
                "merge_timestamp": str(torch.datetime.now()),
                "original_model_type": "Qwen2MoeForCausalLM",
                "merged_layers": [],  # è¿™ä¸ªå¯ä»¥åœ¨è°ƒç”¨æ—¶å¡«å……
                "merging_method": "svd",
                "total_parameters": sum(p.numel() for p in merged_model.parameters()),
                "trainable_parameters": sum(p.numel() for p in merged_model.parameters() if p.requires_grad),
            }
            
            # ç»Ÿè®¡æ¯å±‚çš„ä¸“å®¶æ•°é‡
            moe_layer_info = {}
            for i, layer in enumerate(merged_model.model.layers):
                if isinstance(layer.mlp, Qwen2MoeSparseMoeBlock):
                    moe_layer_info[f"layer_{i}"] = {
                        "num_experts": len(layer.mlp.experts),
                        "is_moe_layer": True
                    }
                else:
                    moe_layer_info[f"layer_{i}"] = {
                        "is_moe_layer": False
                    }
            
            merge_info["moe_layers_info"] = moe_layer_info
            
            config_path = os.path.join(model_path, "merge_info.json")
            with open(config_path, 'w') as f:
                json.dump(merge_info, f, indent=2, default=str)
            
            print(f"ğŸ“ Merge configuration saved to: {config_path}")
        
        # éªŒè¯ä¿å­˜æ˜¯å¦æˆåŠŸ
        print("ğŸ” Validating saved model...")
        saved_files = os.listdir(model_path)
        required_files = ['config.json']
        
        missing_files = [f for f in required_files if f not in saved_files]
        if missing_files:
            print(f"âš ï¸  Warning: Missing files: {missing_files}")
        else:
            print("âœ… All required files saved successfully!")
        
        # è®¡ç®—æ¨¡å‹å¤§å°
        total_size = sum(os.path.getsize(os.path.join(model_path, f)) 
                        for f in saved_files if os.path.isfile(os.path.join(model_path, f)))
        size_gb = total_size / (1024**3)
        
        print(f"ğŸ“Š Model statistics:")
        print(f"   - Total parameters: {sum(p.numel() for p in merged_model.parameters()):,}")
        print(f"   - Model size on disk: {size_gb:.2f} GB")
        print(f"   - Number of files: {len(saved_files)}")
        
        print(f"âœ… Model successfully saved to: {model_path}")
        return model_path
        
    except Exception as e:
        print(f"âŒ Error saving model: {str(e)}")
        import traceback
        traceback.print_exc()
        raise

def main():
    """ä¸»å‡½æ•°ï¼šæ‰§è¡Œä¸“å®¶åˆå¹¶"""
    # é…ç½®å‚æ•°
    MODEL_PATH = "/root/fsas/models/Qwen/Qwen1.5-MoE-A2.7B"
    CLUSTER_DIR = "/root/fsas/zhanghongyu/SMoE/qwen/analysis_results"  # èšç±»ç»“æœå­˜æ”¾ä½ç½®
    RESULT_DIR = "/root/fsas/zhanghongyu/SMoE/qwen/analysis_results"   # æ¿€æ´»é¢‘ç‡å­˜æ”¾ä½ç½®
    OUTPUT_DIR = "/root/fsas/zhanghongyu/SMoE/qwen/merged_models"
    
    # è¦åˆå¹¶çš„MoEå±‚ (Qwen1.5-MoEçš„MoEå±‚é€šå¸¸æ˜¯å¥‡æ•°å±‚)
    TARGET_LAYERS = [1, 3, 5, 7, 9]  
    MERGING_METHOD = "svd"  # å¯é€‰: "svd" æˆ– "frequency"
    
    try:
        print("ğŸš€ Starting Expert Merging Pipeline")
        print(f"{'='*60}")
        
        # åŠ è½½åŸå§‹æ¨¡å‹å’Œåˆ†è¯å™¨
        print("ğŸ“‚ Loading original model and tokenizer...")
        from transformers import AutoTokenizer
        
        model = Qwen2MoeForCausalLM.from_pretrained(
            MODEL_PATH,
            torch_dtype=torch.bfloat16,
            device_map="cpu",  # åœ¨CPUä¸Šè¿›è¡Œåˆå¹¶ä»¥èŠ‚çœæ˜¾å­˜
            trust_remote_code=True
        )
        
        tokenizer = AutoTokenizer.from_pretrained(
            MODEL_PATH,
            trust_remote_code=True
        )
        
        print(f"âœ… Model loaded: {type(model).__name__}")
        print(f"âœ… Tokenizer loaded: {type(tokenizer).__name__}")
        
        # æ‰§è¡Œä¸“å®¶åˆå¹¶
        merged_model = merge_model_experts(
            model=model,
            cluster_dir=CLUSTER_DIR,
            result_dir=RESULT_DIR,
            target_layers=TARGET_LAYERS,
            merging_method=MERGING_METHOD
        )
        
        # ä¿å­˜åˆå¹¶åçš„æ¨¡å‹
        model_name = f"qwen1.5_moe_merged_{MERGING_METHOD}_layers_{'_'.join(map(str, TARGET_LAYERS))}"
        saved_path = save_merged_model(
            merged_model=merged_model,
            tokenizer=tokenizer,
            output_dir=OUTPUT_DIR,
            model_name=model_name
        )
        
        print(f"\nğŸ‰ Expert merging pipeline completed successfully!")
        print(f"ğŸ¯ Merged model saved to: {saved_path}")
        
    except Exception as e:
        print(f"ğŸ’¥ Fatal error during merging: {e}")
        import traceback
        traceback.print_exc()
        raise

if __name__ == "__main__":
    main()