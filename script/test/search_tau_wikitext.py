import torch
import numpy as np
from datasets import load_dataset
from transformers import AutoModelForCausalLM, AutoTokenizer
from torch.utils.data import DataLoader
from tqdm import tqdm
import torch.nn.functional as F
import json
import os
import sys
from transformers.models.qwen2_moe.modeling_qwen2_moe import Qwen2MoeSparseMoeBlock, Qwen2MoeMLP, Qwen2MoeForCausalLM, Qwen2MoeDecoderLayer
from typing import cast, List, Dict, Tuple

project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "../.."))
sys.path.append(project_root)

os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
# print(project_root)

from src import config

# --- 1. é…ç½® ---
# ä» evaluate_benchmark.sh ä¸­æå–çš„è·¯å¾„ä¿¡æ¯
MODEL_NAME = "Qwen/expert_svd_router_avg_k45"
MODEL_PATH = "/root/fsas/zhanghongyu/LAMoE/models/Qwen/expert_svd_router_avg_k45"
OUTPUT_DIR = os.path.join(config.EVALUATE_DIR, "calibration_results")

# éªŒè¯çš„é…ç½®
VALIDATION_DATASET = "wikitext"
VALIDATION_SUBSET = "wikitext-2-raw-v1"
NUM_VALIDATION_SAMPLES = 256
SEQUENCE_LENGTH = config.MAX_LENGTH
BATCH_SIZE = 4 # æ ¹æ®æ‚¨çš„ GPU æ˜¾å­˜è°ƒæ•´
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# æœç´¢èŒƒå›´
TAU_RANGE = np.arange(1.9, 2.1, 0.01).tolist() # ä¾‹å¦‚ [0.0, 0.1, 0.2, ..., 1.5]

# --- 2. è¾…åŠ©å‡½æ•°å’Œç±» ---
class LogitAdjustmentHook:
    """
    ä¸€ä¸ª PyTorch Hook ç±»ï¼Œç”¨äºåœ¨ router çš„ forward pass ååŠ¨æ€è°ƒæ•´ logitsã€‚
    """
    def __init__(self, num_experts, device):
        self.tau = 0.0
        self.log_freqs = torch.zeros(num_experts, device=device)

    def set_params(self, tau, expert_freqs):
        self.tau = tau
        # é¿å… log(0)
        self.log_freqs = torch.log(expert_freqs + 1e-9).to(DEVICE)

    def __call__(self, module, input, output):
        # output æ˜¯ router çš„è¾“å‡ºï¼Œé€šå¸¸æ˜¯ä¸€ä¸ªå…ƒç»„ (router_logits, ...)
        original_logits = output
        
        # åº”ç”¨ Logit Adjustment
        adjusted_logits = original_logits + self.tau * self.log_freqs
        
        return adjusted_logits

def prepare_dataset(dataset_name, subset, num_samples, tokenizer, seq_length):
    """åŠ è½½å¹¶é¢„å¤„ç†æ•°æ®é›†"""
    dataset = load_dataset(
        path=dataset_name, 
        name=subset, 
        split="test", 
        cache_dir=config.DATASET_CACHE_DIR,
    )
    text_list = [item['text'] for item in dataset.select(range(num_samples))]
    
    all_tokens = []
    for text in text_list:
        if text:
            tokens = tokenizer.encode(text)
            all_tokens.extend(tokens)
            
    # å°†æ‰€æœ‰æ–‡æœ¬æ‹¼æ¥åï¼ŒæŒ‰å›ºå®šé•¿åº¦åˆ‡å—
    token_chunks = []
    for i in range(0, len(all_tokens), seq_length):
        chunk = all_tokens[i:i+seq_length]
        if len(chunk) == seq_length:
            token_chunks.append({"input_ids": chunk})
            
    return token_chunks

# --- 3. ä¸»é€»è¾‘ ---

def main():
    print("ğŸš€ å¼€å§‹ Post-hoc Calibration æµç¨‹...")
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    # --- æ­¥éª¤ 1: åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ ---
    print(f"åŠ è½½æ¨¡å‹: {MODEL_PATH}")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True, local_files_only=True)
    model = cast(Qwen2MoeForCausalLM, AutoModelForCausalLM.from_pretrained(
        MODEL_PATH,
        dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True,
        local_files_only=True,
    ))
    model.eval()
    
    try:
        num_experts = model.config.num_experts
        print(f"âœ… ä»æ¨¡å‹é…ç½®ä¸­è·å–ä¸“å®¶æ•°é‡: {num_experts}")
    except AttributeError:
        print("âŒ é”™è¯¯: æ— æ³•ä»æ¨¡å‹é…ç½®ä¸­è·å–ä¸“å®¶æ•°é‡ã€‚")
        return

    # --- æ­¥éª¤ 2: åŠ è½½æ¯å±‚ç‹¬ç«‹çš„ä¸“å®¶é¢‘ç‡ ---
    print(f"\nğŸ“Š ä» '{config.FREQ_RESULT_DIR}' åŠ è½½å„å±‚ç‹¬ç«‹çš„ä¸“å®¶æ¿€æ´»é¢‘ç‡...")
    
    layer_specific_freqs: Dict[int, torch.Tensor] = {}
    layers_to_process = config.TARGET_LAYERS
    if not layers_to_process:
        print("âŒ é”™è¯¯: config.TARGET_LAYERS ä¸ºç©ºï¼Œè¯·æŒ‡å®šè¦åˆ†æçš„å±‚ã€‚")
        return
        
    print(f"å°†ä¸ºä»¥ä¸‹å±‚åŠ è½½é¢‘ç‡: {layers_to_process}")

    for layer_idx in layers_to_process:
        freq_file_path = os.path.join(config.FREQ_RESULT_DIR, f"activation_frequency_layer_{layer_idx}.pt")
        
        if not os.path.exists(freq_file_path):
            print(f"âš ï¸ è­¦å‘Š: é¢‘ç‡æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡å±‚ {layer_idx}: {freq_file_path}")
            continue

        freq_data = torch.load(freq_file_path, map_location=DEVICE)
        
        if 'activation_counts' not in freq_data:
            print(f"âš ï¸ è­¦å‘Š: æ–‡ä»¶ '{freq_file_path}' ä¸­æ²¡æœ‰æ‰¾åˆ° 'activation_counts' é”®ï¼Œè·³è¿‡å±‚ {layer_idx}ã€‚")
            continue
            
        layer_counts = freq_data['activation_counts']
        
        if layer_counts.shape[0] != num_experts:
            print(f"âŒ é”™è¯¯: ç¬¬ {layer_idx} å±‚çš„é¢‘ç‡å¼ é‡ç»´åº¦ ({layer_counts.shape[0]}) ä¸æ¨¡å‹ä¸“å®¶æ•°é‡ ({num_experts}) ä¸åŒ¹é…ã€‚")
            return
        
        if layer_counts.sum() > 0:
            layer_specific_freqs[layer_idx] = layer_counts.float() / layer_counts.sum()
        else:
            print(f"âš ï¸ è­¦å‘Š: ç¬¬ {layer_idx} å±‚æ€»æ¿€æ´»è®¡æ•°ä¸º 0ï¼Œå°†æ— æ³•ä½¿ç”¨ã€‚")

    if not layer_specific_freqs:
        print("âŒ é”™è¯¯: æœªèƒ½æˆåŠŸåŠ è½½ä»»ä½•å±‚çš„æœ‰æ•ˆé¢‘ç‡æ•°æ®ã€‚è¯·æ£€æŸ¥ FREQ_RESULT_DIR è·¯å¾„å’Œæ–‡ä»¶å†…å®¹ã€‚")
        return
    
    print(f"âœ… æˆåŠŸä¸º {len(layer_specific_freqs)} ä¸ªå±‚åŠ è½½äº†é¢‘ç‡æ•°æ®ã€‚")

    # --- æ­¥éª¤ 3: ä¸ºæ¯ä¸ªç›®æ ‡å±‚æ³¨å†Œ Hook ---
    hooks: List[Tuple[int, LogitAdjustmentHook]] = []
    handles = []
    print("\nğŸ”§ ä¸ºæ¯ä¸ªç›®æ ‡ MoE å±‚æ³¨å†Œ Hook...")
    for i, layer in enumerate(model.model.layers):
        # åªä¸ºæˆåŠŸåŠ è½½äº†é¢‘ç‡çš„å±‚æ³¨å†Œhook
        if i in layer_specific_freqs:
            try:
                router_module = cast(Qwen2MoeDecoderLayer, layer).mlp.gate
                hook = LogitAdjustmentHook(num_experts, DEVICE)
                handle = router_module.register_forward_hook(hook)
                
                hooks.append((i, hook)) # ä¿å­˜å±‚ç´¢å¼•å’Œhookå®ä¾‹
                handles.append(handle)
                print(f"  - å·²åœ¨ç¬¬ {i} å±‚æ³¨å†Œ Hookã€‚")
            except AttributeError:
                print(f"âš ï¸ è­¦å‘Š: æ— æ³•åœ¨ç¬¬ {i} å±‚æ‰¾åˆ° 'mlp.gate'ï¼Œè·³è¿‡è¯¥å±‚ã€‚")
    
    if not handles:
        print("âŒ é”™è¯¯: æœªèƒ½æˆåŠŸæ³¨å†Œä»»ä½• Hookã€‚")
        return

    # --- æ­¥éª¤ 4: å‡†å¤‡éªŒè¯é›† ---
    validation_data = prepare_dataset(
        VALIDATION_DATASET, VALIDATION_SUBSET, NUM_VALIDATION_SAMPLES, tokenizer, SEQUENCE_LENGTH
    )
    validation_loader = DataLoader(validation_data, batch_size=BATCH_SIZE)

    # --- æ­¥éª¤ 5: Grid Search tau å¹¶è¯„ä¼° PPL å’Œ Entropy ---
    print(f"\nğŸ” å¼€å§‹åœ¨éªŒè¯é›†ä¸Šæœç´¢æœ€ä½³ tauï¼ŒèŒƒå›´: {TAU_RANGE}")
    results = []

    for tau in TAU_RANGE:
        # ä¸ºæ¯ä¸ªhookè®¾ç½®å…¶å¯¹åº”å±‚çš„é¢‘ç‡å’Œå½“å‰çš„tau
        for layer_idx, hook in hooks:
            hook.set_params(tau, layer_specific_freqs[layer_idx])
        
        total_loss = 0
        total_tokens = 0
        total_entropy = 0
        
        with torch.no_grad():
            for batch in tqdm(validation_loader, desc=f"è¯„ä¼° Tau={tau:.2f}"):
                input_ids = torch.stack(batch["input_ids"]).to(DEVICE)
                labels = input_ids.clone()
                
                outputs = model(input_ids, labels=labels, output_router_logits=True)
                
                # è®¡ç®— Perplexity
                loss = outputs.loss
                total_loss += loss.item() * (input_ids.size(0) * input_ids.size(1))
                total_tokens += input_ids.size(0) * input_ids.size(1)

                # è®¡ç®— Router Entropy
                # outputs.router_logits æ˜¯ä¸€ä¸ªå…ƒç»„ï¼Œæ¯ä¸ªå…ƒç´ å¯¹åº”ä¸€å±‚MoEçš„è¾“å‡º
                # æ¯ä¸ªå…ƒç´ åˆæ˜¯ (router_logits, expert_indices) çš„å…ƒç»„
                if outputs.router_logits:
                    all_router_logits = torch.cat([l[0] for l in outputs.router_logits], dim=0)
                    probs = F.softmax(all_router_logits, dim=-1)
                    log_probs = F.log_softmax(all_router_logits, dim=-1)
                    entropy = -torch.sum(probs * log_probs, dim=-1).mean()
                    total_entropy += entropy.item()

        avg_loss = total_loss / total_tokens
        perplexity = np.exp(avg_loss)
        avg_entropy = total_entropy / len(validation_loader) if len(validation_loader) > 0 else 0
        
        print(f"Tau: {tau:.2f} -> Perplexity: {perplexity:.4f}, Router Entropy: {avg_entropy:.4f}")
        results.append({"tau": tau, "perplexity": perplexity, "router_entropy": avg_entropy})

    # --- æ­¥éª¤ 6: é€‰å®šæœ€ä½³ Tau å¹¶ä¿å­˜ç»“æœ ---
    for handle in handles:
        handle.remove() # æ¸…ç†æ‰€æœ‰ hooks
    print("\nâœ… æ‰€æœ‰ Hook å·²è¢«ç§»é™¤ã€‚")

    best_result = min(results, key=lambda x: x["perplexity"])
    print("\nğŸ‰ æ ¡å‡†å®Œæˆï¼")
    print(f"æœ€ä½³ Tau: {best_result['tau']:.2f}")
    print(f"æœ€ä½ Perplexity: {best_result['perplexity']:.4f}")
    print(f"å¯¹åº”çš„ Router Entropy: {best_result['router_entropy']:.4f}")

    # ä¿å­˜è¯¦ç»†ç»“æœ
    output_file = os.path.join(OUTPUT_DIR, "calibration_results.json")
    with open(output_file, "w") as f:
        json.dump(results, f, indent=2)
    print(f"è¯¦ç»†ç»“æœå·²ä¿å­˜è‡³: {output_file}")
    
    print("\nä¸‹ä¸€æ­¥å»ºè®®:")
    print(f"ä½¿ç”¨é€‰å®šçš„ tau = {best_result['tau']:.2f} å‚æ•°ï¼Œåœ¨ä½ çš„ `evaluate_benchmark.sh` ä¸­è¿›è¡Œä¸€æ¬¡å®Œæ•´çš„è¯„ä¼°ã€‚")


if __name__ == "__main__":
    main()