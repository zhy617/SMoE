import torch
import numpy as np
from datasets import load_dataset
from transformers import AutoModelForCausalLM, AutoTokenizer
from torch.utils.data import DataLoader
from tqdm import tqdm
import torch.nn.functional as F
import json
import os
import sys
from transformers.models.qwen2_moe.modeling_qwen2_moe import Qwen2MoeSparseMoeBlock, Qwen2MoeMLP, Qwen2MoeForCausalLM, Qwen2MoeDecoderLayer
from typing import cast, List, Dict, Tuple

project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "../.."))
sys.path.append(project_root)

os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
# print(project_root)

from src import config

# --- 1. é…ç½® ---
# ä» evaluate_benchmark.sh ä¸­æå–çš„è·¯å¾„ä¿¡æ¯
MODEL_NAME = "Qwen/expert_svd_router_avg_k45"
MODEL_PATH = "/root/fsas/zhanghongyu/LAMoE/models/Qwen/expert_svd_router_avg_k45"
OUTPUT_DIR = os.path.join(config.EVALUATE_DIR, "calibration_results")

# éªŒè¯çš„é…ç½®
VALIDATION_DATASET = "ai2_arc"
VALIDATION_SUBSET = "ARC-Challenge"
NUM_VALIDATION_SAMPLES = 256
# SEQUENCE_LENGTH = config.MAX_LENGTH
BATCH_SIZE = 1 # æ ¹æ®æ‚¨çš„ GPU æ˜¾å­˜è°ƒæ•´
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# æœç´¢èŒƒå›´
TAU_RANGE = np.arange(-0.3, -0.1, 0.01).tolist() # ä¾‹å¦‚ [0.0, 0.1, 0.2, ..., 1.5]

# --- 2. è¾…åŠ©å‡½æ•°å’Œç±» ---
class LogitAdjustmentHook:
    """
    ä¸€ä¸ª PyTorch Hook ç±»ï¼Œç”¨äºåœ¨ router çš„ forward pass ååŠ¨æ€è°ƒæ•´ logitsã€‚
    """
    def __init__(self, num_experts, device):
        self.tau = 0.0
        self.log_freqs = torch.zeros(num_experts, device=device)

    def set_params(self, tau, expert_freqs):
        self.tau = tau
        # é¿å… log(0)
        self.log_freqs = torch.log(expert_freqs + 1e-9).to(DEVICE)

    def __call__(self, module, input, output):
        # output æ˜¯ router çš„è¾“å‡ºï¼Œé€šå¸¸æ˜¯ä¸€ä¸ªå…ƒç»„ (router_logits, ...)
        original_logits = output
        
        # åº”ç”¨ Logit Adjustment
        adjusted_logits = original_logits + self.tau * self.log_freqs
        
        return adjusted_logits

# def prepare_dataset(dataset_name, subset, num_samples, tokenizer, seq_length):
#     """åŠ è½½å¹¶é¢„å¤„ç†æ•°æ®é›†"""
#     dataset = load_dataset(
#         path=dataset_name, 
#         name=subset, 
#         split="test", 
#         cache_dir=config.DATASET_CACHE_DIR,
#     )
#     text_list = [item['text'] for item in dataset.select(range(num_samples))]
    
#     all_tokens = []
#     for text in text_list:
#         if text:
#             tokens = tokenizer.encode(text)
#             all_tokens.extend(tokens)
            
#     # å°†æ‰€æœ‰æ–‡æœ¬æ‹¼æ¥åï¼ŒæŒ‰å›ºå®šé•¿åº¦åˆ‡å—
#     token_chunks = []
#     for i in range(0, len(all_tokens), seq_length):
#         chunk = all_tokens[i:i+seq_length]
#         if len(chunk) == seq_length:
#             token_chunks.append({"input_ids": chunk})
            
#     return token_chunks

def prepare_arc_for_tcll(dataset_name, subset, num_samples, tokenizer):
    """
    åŠ è½½å¹¶é¢„å¤„ç† ARC æ•°æ®é›†ï¼Œä¸º TCLL è®¡ç®—åšå‡†å¤‡ã€‚
    """
    dataset = load_dataset(
        path=dataset_name,
        name=subset,
        split="test",
    ).select(range(num_samples))

    processed_samples = []
    for item in dataset:
        question = item['question']
        choices = item['choices']
        answer_key = item['answerKey']

        # æ„å»º prompt
        prompt = f"Question: {question}\nChoices:\n"
        choice_map = {}
        for i, (label, text) in enumerate(zip(choices['label'], choices['text'])):
            prompt += f"{label}. {text}\n"
            choice_map[label] = text

        prompt += "Answer:"
        
        # æ‰¾åˆ°æ­£ç¡®ç­”æ¡ˆå¯¹åº”çš„ Token ID
        # æ³¨æ„ï¼šæˆ‘ä»¬åªå…³å¿ƒæ¨¡å‹ç”Ÿæˆçš„ç¬¬ä¸€ä¸ª token æ˜¯å¦æ˜¯æ­£ç¡®ç­”æ¡ˆçš„æ ‡ç­¾
        correct_choice_label = answer_key
        # å°† 'A', 'B', 'C', 'D' ç­‰æ ‡ç­¾è½¬æ¢ä¸º token ID
        # åŠ ä¸€ä¸ªç©ºæ ¼å‰ç¼€ ' A' ä»¥åŒ¹é…æ¨¡å‹ç”Ÿæˆä¹ æƒ¯
        target_token_id = tokenizer.encode(f" {correct_choice_label}")[0]

        input_ids = tokenizer.encode(prompt, return_tensors="pt")

        processed_samples.append({
            "input_ids": input_ids,
            "target_token_id": target_token_id
        })
        
    return processed_samples

# --- 3. ä¸»é€»è¾‘ ---

def main():
    print("ğŸš€ å¼€å§‹ Post-hoc Calibration æµç¨‹...")
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    # --- æ­¥éª¤ 1: åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ ---
    print(f"åŠ è½½æ¨¡å‹: {MODEL_PATH}")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True, local_files_only=True)
    model = cast(Qwen2MoeForCausalLM, AutoModelForCausalLM.from_pretrained(
        MODEL_PATH,
        dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True,
        local_files_only=True,
    ))
    model.eval()
    
    try:
        num_experts = model.config.num_experts
        print(f"âœ… ä»æ¨¡å‹é…ç½®ä¸­è·å–ä¸“å®¶æ•°é‡: {num_experts}")
    except AttributeError:
        print("âŒ é”™è¯¯: æ— æ³•ä»æ¨¡å‹é…ç½®ä¸­è·å–ä¸“å®¶æ•°é‡ã€‚")
        return

    # --- æ­¥éª¤ 2: åŠ è½½æ¯å±‚ç‹¬ç«‹çš„ä¸“å®¶é¢‘ç‡ ---
    print(f"\nğŸ“Š ä» '{config.FREQ_RESULT_DIR}' åŠ è½½å„å±‚ç‹¬ç«‹çš„ä¸“å®¶æ¿€æ´»é¢‘ç‡...")
    
    layer_specific_freqs: Dict[int, torch.Tensor] = {}
    layers_to_process = config.TARGET_LAYERS
    if not layers_to_process:
        print("âŒ é”™è¯¯: config.TARGET_LAYERS ä¸ºç©ºï¼Œè¯·æŒ‡å®šè¦åˆ†æçš„å±‚ã€‚")
        return
        
    print(f"å°†ä¸ºä»¥ä¸‹å±‚åŠ è½½é¢‘ç‡: {layers_to_process}")

    for layer_idx in layers_to_process:
        freq_file_path = os.path.join(config.FREQ_RESULT_DIR, f"activation_frequency_layer_{layer_idx}.pt")
        
        if not os.path.exists(freq_file_path):
            print(f"âš ï¸ è­¦å‘Š: é¢‘ç‡æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡å±‚ {layer_idx}: {freq_file_path}")
            continue

        freq_data = torch.load(freq_file_path, map_location=DEVICE)
        
        if 'activation_counts' not in freq_data:
            print(f"âš ï¸ è­¦å‘Š: æ–‡ä»¶ '{freq_file_path}' ä¸­æ²¡æœ‰æ‰¾åˆ° 'activation_counts' é”®ï¼Œè·³è¿‡å±‚ {layer_idx}ã€‚")
            continue
            
        layer_counts = freq_data['activation_counts']
        
        if layer_counts.shape[0] != num_experts:
            print(f"âŒ é”™è¯¯: ç¬¬ {layer_idx} å±‚çš„é¢‘ç‡å¼ é‡ç»´åº¦ ({layer_counts.shape[0]}) ä¸æ¨¡å‹ä¸“å®¶æ•°é‡ ({num_experts}) ä¸åŒ¹é…ã€‚")
            return
        
        if layer_counts.sum() > 0:
            layer_specific_freqs[layer_idx] = layer_counts.float() / layer_counts.sum()
        else:
            print(f"âš ï¸ è­¦å‘Š: ç¬¬ {layer_idx} å±‚æ€»æ¿€æ´»è®¡æ•°ä¸º 0ï¼Œå°†æ— æ³•ä½¿ç”¨ã€‚")

    if not layer_specific_freqs:
        print("âŒ é”™è¯¯: æœªèƒ½æˆåŠŸåŠ è½½ä»»ä½•å±‚çš„æœ‰æ•ˆé¢‘ç‡æ•°æ®ã€‚è¯·æ£€æŸ¥ FREQ_RESULT_DIR è·¯å¾„å’Œæ–‡ä»¶å†…å®¹ã€‚")
        return
    
    print(f"âœ… æˆåŠŸä¸º {len(layer_specific_freqs)} ä¸ªå±‚åŠ è½½äº†é¢‘ç‡æ•°æ®ã€‚")

    # --- æ­¥éª¤ 3: ä¸ºæ¯ä¸ªç›®æ ‡å±‚æ³¨å†Œ Hook ---
    hooks: List[Tuple[int, LogitAdjustmentHook]] = []
    handles = []
    print("\nğŸ”§ ä¸ºæ¯ä¸ªç›®æ ‡ MoE å±‚æ³¨å†Œ Hook...")
    for i, layer in enumerate(model.model.layers):
        # åªä¸ºæˆåŠŸåŠ è½½äº†é¢‘ç‡çš„å±‚æ³¨å†Œhook
        if i in layer_specific_freqs:
            try:
                router_module = cast(Qwen2MoeDecoderLayer, layer).mlp.gate
                hook = LogitAdjustmentHook(num_experts, DEVICE)
                handle = router_module.register_forward_hook(hook)
                
                hooks.append((i, hook)) # ä¿å­˜å±‚ç´¢å¼•å’Œhookå®ä¾‹
                handles.append(handle)
                print(f"  - å·²åœ¨ç¬¬ {i} å±‚æ³¨å†Œ Hookã€‚")
            except AttributeError:
                print(f"âš ï¸ è­¦å‘Š: æ— æ³•åœ¨ç¬¬ {i} å±‚æ‰¾åˆ° 'mlp.gate'ï¼Œè·³è¿‡è¯¥å±‚ã€‚")
    
    if not handles:
        print("âŒ é”™è¯¯: æœªèƒ½æˆåŠŸæ³¨å†Œä»»ä½• Hookã€‚")
        return

 # --- æ­¥éª¤ 4: å‡†å¤‡éªŒè¯é›† (å·²ä¿®æ”¹) ---
    print("\nğŸ“š å‡†å¤‡ ARC éªŒè¯é›†ç”¨äº TCLL è¯„ä¼°...")
    validation_data = prepare_arc_for_tcll(
        VALIDATION_DATASET, VALIDATION_SUBSET, NUM_VALIDATION_SAMPLES, tokenizer
    )
    # æ³¨æ„ï¼šTCLL ä¸éœ€è¦ DataLoaderï¼Œå› ä¸ºæˆ‘ä»¬é€ä¸ªå¤„ç†
    
    # --- æ­¥éª¤ 5: Grid Search tau å¹¶è¯„ä¼° TCLL (å·²ä¿®æ”¹) ---
    print(f"\nğŸ” å¼€å§‹åœ¨ ARC éªŒè¯é›†ä¸Šæœç´¢æœ€ä½³ tauï¼ŒèŒƒå›´: {TAU_RANGE}")
    results = []

    for tau in TAU_RANGE:
        for layer_idx, hook in hooks:
            hook.set_params(tau, layer_specific_freqs[layer_idx])
        
        total_tcll = 0.0
        
        with torch.no_grad():
            for sample in tqdm(validation_data, desc=f"è¯„ä¼° Tau={tau:.2f}"):
                input_ids = sample["input_ids"].to(DEVICE)
                target_token_id = sample["target_token_id"]

                # è·å–æ¨¡å‹åœ¨æœ€åä¸€ä¸ªä½ç½®çš„ logits
                outputs = model(input_ids)
                # shape: [batch_size, seq_len, vocab_size] -> [1, seq_len, vocab_size]
                last_token_logits = outputs.logits[0, -1, :]

                # è®¡ç®— log_softmax ä»¥è·å¾—å¯¹æ•°æ¦‚ç‡
                log_probs = F.log_softmax(last_token_logits, dim=-1)

                # æå–æ­£ç¡®ç­”æ¡ˆ token çš„å¯¹æ•°æ¦‚ç‡
                tcll = log_probs[target_token_id].item()
                total_tcll += tcll

        avg_tcll = total_tcll / len(validation_data)
        
        print(f"Tau: {tau:.2f} -> å¹³å‡TCLL: {avg_tcll:.4f}")
        results.append({"tau": tau, "avg_tcll": avg_tcll})

    # --- æ­¥éª¤ 6: é€‰å®šæœ€ä½³ Tau å¹¶ä¿å­˜ç»“æœ (å·²ä¿®æ”¹) ---
    for handle in handles:
        handle.remove()
    print("\nâœ… æ‰€æœ‰ Hook å·²è¢«ç§»é™¤ã€‚")

    # TCLL çš„ç›®æ ‡æ˜¯æœ€å¤§åŒ–ï¼Œæ‰€ä»¥æˆ‘ä»¬æ‰¾ avg_tcll æœ€å¤§çš„ç»“æœ
    best_result = max(results, key=lambda x: x["avg_tcll"])
    print("\nğŸ‰ æ ¡å‡†å®Œæˆï¼")
    print(f"æœ€ä½³ Tau: {best_result['tau']:.2f}")
    print(f"æœ€é«˜å¹³å‡ TCLL: {best_result['avg_tcll']:.4f}")

    output_file = os.path.join(OUTPUT_DIR, "calibration_results_tcll.json")
    with open(output_file, "w") as f:
        json.dump(results, f, indent=2)
    print(f"è¯¦ç»†ç»“æœå·²ä¿å­˜è‡³: {output_file}")
    
    print("\nä¸‹ä¸€æ­¥å»ºè®®:")
    print(f"ä½¿ç”¨é€‰å®šçš„ tau = {best_result['tau']:.2f} å‚æ•°ï¼Œåœ¨ä½ çš„ `evaluate_benchmark.sh` ä¸­è¿›è¡Œä¸€æ¬¡å®Œæ•´çš„è¯„ä¼°ã€‚")


if __name__ == "__main__":
    main()